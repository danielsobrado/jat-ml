Files:
File: rag/tests/langgraph_vis/test_simulation_delay.py


File: rag/config.py
"""Configuration loader for the application."""
import os
import yaml
from typing import Dict, Any, Optional
import logging
from dataclasses import dataclass, field # Import field
from pathlib import Path # Added Path

# Set up basic logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("config")

@dataclass
class ServerConfig:
    host: str
    port: int
    log_level: str
    environment: str

@dataclass
class ChromaDBConfig:
    host: str
    port: int
    default_collection: str
    manual_info_collection: str 
    unspsc_collection: str
    common_collection: str
@dataclass
class PostgreSQLConfig:
    host: str
    port: int
    user: str
    password: str
    dbname: str

@dataclass
class AuthConfig:
    enabled: bool
    secret_key: str
    token_expire_minutes: int
    default_admin: Dict[str, str]

@dataclass
class AppConfig:
    server: ServerConfig
    chromadb: ChromaDBConfig
    auth: AuthConfig
    postgres: PostgreSQLConfig 

def load_config(config_path: Optional[str] = None) -> AppConfig:
    """Load configuration from YAML file and environment variables.
    
    Args:
        config_path: Path to YAML config file (optional, defaults to config.yaml in cwd)
        
    Returns:
        AppConfig: Application configuration
    """
    # Determine the directory of the current script (config.py)
    _current_script_dir = Path(__file__).resolve().parent # e.g., c:/Dev/jat/jat-ml/rag
    _project_root_dir = _current_script_dir.parent # e.g., c:/Dev/jat/jat-ml
    _default_yaml_path = _project_root_dir / "config" / "config.yaml"

    # Default config file path
    if config_path is None:
        config_path = os.environ.get("CONFIG_PATH", str(_default_yaml_path))
    
    logger.info(f"Attempting to load configuration from: {config_path}")
    logger.info(f"Default YAML path determined by script location: {_default_yaml_path}")
    env_var_config_path = os.environ.get("CONFIG_PATH")
    logger.info(f"CONFIG_PATH environment variable: {env_var_config_path}")


    # Load config from YAML
    config_data = {}
    try:
        # Try absolute path first
        abs_config_path = os.path.abspath(config_path)
        logger.debug(f"Absolute config path: {abs_config_path}")
        with open(abs_config_path, "r") as f:
            config_data = yaml.safe_load(f)
            logger.info(f"Successfully loaded config from absolute path: {abs_config_path}")
    except FileNotFoundError:
         logger.warning(f"Config file not found at absolute path {abs_config_path}. Trying relative path {config_path}")
         try:
              with open(config_path, "r") as f:
                  config_data = yaml.safe_load(f)
                  logger.info(f"Successfully loaded config from relative path: {config_path}")
         except FileNotFoundError:
              logger.warning(f"Config file not found at {config_path}. Using default values and environment variables.")
         except Exception as e:
              logger.error(f"Error loading config file from relative path {config_path}: {e}")
    except Exception as e:
        logger.error(f"Error loading config file from absolute path {abs_config_path}: {e}")

    if not isinstance(config_data, dict): # Handle empty or invalid YAML
        logger.warning(f"Config data loaded from {config_path} is not a dictionary or is empty. Relying on defaults/env vars.")
        config_data = {}


    # Server config with environment variable overrides
    server_config_yaml = config_data.get("server", {})
    server = ServerConfig(
        host=os.environ.get("SERVER_HOST", server_config_yaml.get("host", "0.0.0.0")),
        port=int(os.environ.get("SERVER_PORT", server_config_yaml.get("port", 8080))),
        log_level=os.environ.get("LOG_LEVEL", server_config_yaml.get("log_level", "INFO")).upper(),
        environment=os.environ.get("ENVIRONMENT", server_config_yaml.get("environment", "production"))
    )
    
    # ChromaDB config with environment variable overrides
    chromadb_config_yaml = config_data.get("chromadb", {})
    chromadb = ChromaDBConfig(
        host=os.environ.get("CHROMA_HOST", chromadb_config_yaml.get("host", "localhost")),
        port=int(os.environ.get("CHROMA_PORT", chromadb_config_yaml.get("port", 8000))),
        default_collection=os.environ.get(
            "DEFAULT_COLLECTION", 
            chromadb_config_yaml.get("default_collection", "unspsc_categories")
        ),
        # Add manual_info_collection loading
        manual_info_collection=os.environ.get(
            "MANUAL_INFO_COLLECTION",
            chromadb_config_yaml.get("manual_info_collection", "rag_manual_info") # Default name
        ),
        # Add unspsc_collection and common_collection loading
        unspsc_collection=os.environ.get(
            "UNSPSC_COLLECTION",
            chromadb_config_yaml.get("unspsc_collection", "unspsc_categories")
        ),
        common_collection=os.environ.get(
            "COMMON_COLLECTION",
            chromadb_config_yaml.get("common_collection", "common_categories")
        )
    )
    
    # Auth config with environment variable overrides
    auth_config_yaml = config_data.get("auth", {})
    default_admin_yaml = auth_config_yaml.get("default_admin", {})
    
    # Log the source of the 'enabled' flag for auth
    auth_enabled_from_yaml = auth_config_yaml.get("enabled", "NOT_FOUND_IN_YAML")
    logger.info(f"Auth 'enabled' from YAML ({config_path}): {auth_enabled_from_yaml}")
    
    env_enable_auth = os.environ.get("ENABLE_AUTH")
    logger.info(f"ENABLE_AUTH environment variable: {env_enable_auth}")

    final_auth_enabled_value = str(os.environ.get("ENABLE_AUTH", auth_config_yaml.get("enabled", True))).lower() == "true"
    logger.info(f"Final calculated auth.enabled value: {final_auth_enabled_value} (Source YAML: {auth_enabled_from_yaml}, Env Var ENABLE_AUTH: {env_enable_auth})")

    auth = AuthConfig(
        enabled=final_auth_enabled_value, # Ensure YAML 'enabled' is primary source before env var
        secret_key=os.environ.get("SECRET_KEY", auth_config_yaml.get("secret_key", "CHANGE_THIS_TO_A_SECURE_SECRET")),
        token_expire_minutes=int(os.environ.get(
            "ACCESS_TOKEN_EXPIRE_MINUTES", 
            auth_config_yaml.get("token_expire_minutes", 30)
        )),
        default_admin={
            "username": os.environ.get("ADMIN_USERNAME", default_admin_yaml.get("username", "admin")),
            "password": os.environ.get("ADMIN_PASSWORD", default_admin_yaml.get("password", "admin"))
        }
    )
    
    # PostgreSQL config with environment variable overrides
    postgres_config_yaml = config_data.get("postgres", {})
    postgres = PostgreSQLConfig(
        host=os.environ.get("PG_HOST", postgres_config_yaml.get("host", "localhost")),
        port=int(os.environ.get("PG_PORT", postgres_config_yaml.get("port", 5433))),
        user=os.environ.get("PG_USER", postgres_config_yaml.get("user", "unspsc")),
        password=os.environ.get("PG_PASSWORD", postgres_config_yaml.get("password", "unspsc")),
        dbname=os.environ.get("PG_DBNAME", postgres_config_yaml.get("dbname", "unspsc"))
    )
    
    # Log loaded config (mask sensitive info if needed)
    logger.info("Configuration Loaded:")
    logger.info(f"  Server: host={server.host}, port={server.port}, log_level={server.log_level}, env={server.environment}")
    logger.info(f"  ChromaDB: host={chromadb.host}, port={chromadb.port}, default_collection='{chromadb.default_collection}', manual_info_collection='{chromadb.manual_info_collection}', unspsc_collection='{chromadb.unspsc_collection}', common_collection='{chromadb.common_collection}'")
    logger.info(f"  PostgreSQL: host={postgres.host}, port={postgres.port}, user='{postgres.user}', dbname='{postgres.dbname}'")
    logger.info(f"  Auth: enabled={auth.enabled}, token_expire={auth.token_expire_minutes}m, admin_user='{auth.default_admin['username']}'")
    if not auth.secret_key or auth.secret_key == "CHANGE_THIS_TO_A_SECURE_SECRET":
         logger.warning("Auth secret_key is not set or is using the default insecure value. Please set a strong SECRET_KEY environment variable or update config.yaml.")

    return AppConfig(server=server, chromadb=chromadb, auth=auth, postgres=postgres)

# Global config instance
config = load_config()

def get_server_settings(self) -> ServerConfig:
    """Get server settings from configuration"""
    server_config_yaml = self.config_yaml.get("server", {})
    
    return ServerConfig(
        host=server_config_yaml.get("host", "0.0.0.0"),
        port=int(os.environ.get("SERVER_PORT", server_config_yaml.get("port", 8090))),
        environment=server_config_yaml.get("environment", "production"),
        log_level=server_config_yaml.get("log_level", "INFO"),
    )

File: rag/main.py
from fastapi import FastAPI
import uvicorn
from loguru import logger
import os
import argparse

# Import the app from __init__.py
from . import app

# Configure logging
logger.add("rag_service.log", rotation="500 MB", level="INFO")

if __name__ == "__main__":
    logger.info("Starting Agentic Workflows service...")
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run the Agentic Workflows service")
    parser.add_argument("--server-port", type=int, default=8090, help="Port for the server")
    parser.add_argument("--server-host", type=str, default="0.0.0.0", help="Host for the server")
    args = parser.parse_args()
    
    try:
        # Get port from command line args or environment variable or default
        port = args.server_port
        host = args.server_host
        
        logger.info(f"Starting server on {host}:{port}")
        
        uvicorn.run(
            app,
            host=host,
            port=port,
            log_level="info"
        )
    except Exception as e:
        logger.error(f"Failed to start RAG service: {str(e)}")
        raise

File: rag/__init__.py
# RAG Package initialization
from .api.app import app

# Import main module to register routes
from . import main


File: rag/tests/__init__.py
# rag/tests/__init__.py
# This file makes 'tests' a Python package within the 'rag' package.

File: rag/utils/logging.py
# Logging utilities 


File: rag/utils/__init__.py
# Utilities package 


File: rag/tests/langgraph_vis/test_websocket.py
#!/usr/bin/env python
# test_websocket.py - Simple script to test WebSocket connection and execution
import asyncio
import websockets
import json
import logging
import sys

logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                    stream=sys.stdout)

logger = logging.getLogger("test_websocket")

async def test_websocket(graph_id, initial_args=None, simulation_delay_ms=None):
    """Test WebSocket connection and execution for a graph."""
    if initial_args is None:
        initial_args = {}
    
    # Build the URL (adjust as needed for your environment)
    # This matches the URL in the LangGraphSocketService
    ws_url = f"ws://localhost:8090/v1/lg-vis/ws/langgraph/graphs/{graph_id}/execute"
    logger.info(f"Connecting to WebSocket at: {ws_url}")
    
    try:
        async with websockets.connect(ws_url) as websocket:
            logger.info("WebSocket connected!")
            
            # Prepare initial message
            initial_message = {
                "input_args": initial_args,
            }
            if simulation_delay_ms is not None:
                initial_message["simulation_delay_ms"] = simulation_delay_ms
                
            # Send initial message
            logger.info(f"Sending initial message: {initial_message}")
            await websocket.send(json.dumps(initial_message))
            
            # Listen for messages
            while True:
                try:
                    response = await websocket.recv()
                    event = json.loads(response)
                    logger.info(f"Received event: {event['eventType'] if 'eventType' in event else 'unknown'}")
                    logger.debug(f"Event details: {event}")
                    
                    # If graph execution ends, we're done
                    if event.get('eventType') == 'graph_execution_end':
                        logger.info("Graph execution completed successfully!")
                        break
                    # If we get an error, also exit
                    elif event.get('eventType') == 'graph_error':
                        logger.error(f"Graph execution error: {event.get('message')}")
                        if 'details' in event:
                            logger.error(f"Error details: {event['details']}")
                        break
                except websockets.exceptions.ConnectionClosed:
                    logger.warning("WebSocket connection closed")
                    break
    except Exception as e:
        logger.error(f"Error in WebSocket connection: {e}", exc_info=True)

if __name__ == "__main__":
    # Example usage with a simple graph - update with your graph ID
    graph_id = "example_document_workflow"  # Use a graph ID that exists in your system
    
    # Optional: Provide initial arguments and simulation delay
    initial_args = {}
    simulation_delay_ms = 2000  # 2 seconds delay
    
    asyncio.run(test_websocket(graph_id, initial_args, simulation_delay_ms))


File: rag/tests/langgraph_vis/__init__.py
# rag/tests/langgraph_vis/__init__.py
# This file makes 'langgraph_vis' a Python sub-package within 'rag.tests'.

File: rag/langgraph_vis/schemas.py
# rag/langgraph_vis/schemas.py
from __future__ import annotations # Enables postponed evaluation of type annotations

import uuid
from datetime import datetime
from typing import Any, Dict, List, Literal, Optional

from pydantic import BaseModel, Field, field_validator, ConfigDict
from pydantic.alias_generators import to_camel

# --- Graph Definition Models (for saving/loading graph structures) ---

class NodeUIPosition(BaseModel):
    """Represents the UI position of a node for visualization."""
    x: float = Field(..., description="X-coordinate of the node in the UI.")
    y: float = Field(..., description="Y-coordinate of the node in the UI.")

class NodeDefinition(BaseModel):
    """Defines a single node in the graph structure."""
    id: str = Field(..., description="Unique identifier for the node (e.g., 'llm_formatter').")
    type: str = Field(..., description="Type of the node (e.g., 'llm_node', 'tool_node', 'router_node', 'entry_point', 'end_point'). This maps to backend logic.")
    config: Dict[str, Any] = Field(default_factory=dict, description="Configuration specific to this node type (e.g., LLM prompts, tool names, router logic keys).")
    ui_position: Optional[NodeUIPosition] = Field(None, description="Optional UI positioning hints for frontend rendering.")

class EdgeDefinition(BaseModel):
    """Defines a directed edge between two nodes in the graph structure."""
    id: str = Field(..., description="Unique identifier for the edge (e.g., 'e_llm_to_tool').")
    source: str = Field(..., description="ID of the source node.")
    target: str = Field(..., description="ID of the target node.")
    label: Optional[str] = Field(None, description="Optional label for the edge, often used for conditions or clarity.")
    animated: Optional[bool] = Field(False, description="Hint for UI to animate the edge.")
    # type: Optional[str] = Field(None, description="Optional edge type for custom React Flow rendering (e.g., 'custom_edge').")

class ConditionalEdgeMapping(BaseModel):
    """Defines the target node for a specific condition name."""
    condition_name: str = Field(..., description="The name of the condition (e.g., output value from a router node).")
    target_node_id: str = Field(..., description="The ID of the node to transition to if this condition is met.")

class ConditionalEdgesDefinition(BaseModel):
    """Defines conditional outgoing edges for a specific source node."""
    source_node_id: str = Field(..., description="The ID of the node that has conditional branches.")
    # condition_logic_key: Optional[str] = Field(None, description="Optional: Key in the graph state that the router node's logic uses to determine the condition_name.")
    mappings: List[ConditionalEdgeMapping] = Field(..., description="A list of condition-to-target mappings.")

class GraphDefinition(BaseModel):
    """Full definition of a LangGraph workflow, suitable for serialization (e.g., to JSON)."""
    id: str = Field(default_factory=lambda: f"graph_{uuid.uuid4()}", description="Unique identifier for the graph definition.")
    name: str = Field(..., description="Human-readable name for the graph.")
    description: Optional[str] = Field(None, description="Optional detailed description of the graph's purpose.")
    # LangGraph state schema is Pydantic model. We reference its *name* here.
    # The backend's dynamic_graph_builder will use this name to look up the actual Pydantic model.
    state_schema_name: str = Field(..., description="Name of the Pydantic model representing the LangGraph's state (must be known to the backend).")
    nodes: List[NodeDefinition] = Field(..., description="List of all nodes in the graph.")
    edges: List[EdgeDefinition] = Field(default_factory=list, description="List of all standard (non-conditional) edges.")
    conditional_edges: List[ConditionalEdgesDefinition] = Field(default_factory=list, description="List of conditional edge configurations.")
    entry_point_node_id: str = Field(..., description="ID of the node that serves as the entry point to the graph.")
    # Explicit 'END' nodes are handled by LangGraph. This field can list nodes that directly connect to END.
    # Or, nodes that have no outgoing edges defined here could be implicitly connected to END by the builder.
    # For clarity, let's allow defining nodes that should always transition to END if they are terminal.
    terminal_node_ids: Optional[List[str]] = Field(default_factory=list, description="Optional: List of node IDs that should implicitly transition to the global END state if they don't have other outgoing edges.")
    version: int = Field(1, description="Version of this graph definition structure.")
    created_at: Optional[datetime] = Field(default_factory=datetime.utcnow, description="Timestamp of creation.")
    updated_at: Optional[datetime] = Field(default_factory=datetime.utcnow, description="Timestamp of last update.")

    @field_validator('created_at', 'updated_at', mode='before')
    @classmethod
    def parse_datetime(cls, v):
        # Handle None values
        if v is None:
            return datetime.utcnow()
        # If string, parse it correctly to datetime
        if isinstance(v, str):
            try:
                return datetime.fromisoformat(v.replace('Z', '+00:00'))
            except ValueError:
                try:
                    return datetime.strptime(v, "%Y-%m-%dT%H:%M:%S.%fZ")
                except ValueError:
                    try:
                        return datetime.strptime(v, "%Y-%m-%dT%H:%M:%SZ")
                    except ValueError:
                        return datetime.utcnow()  # Default to current time if parsing fails
        return v

    @field_validator('updated_at', mode='before')
    @classmethod
    def set_updated_at_on_update(cls, v, values): # type: ignore[no-untyped-def]
        # Pydantic v1 style validator, will update to v2 if using model_validator
        # For now, this doesn't automatically update on modification through Pydantic.
        # Actual update logic will be in the API when saving.
        return v or datetime.utcnow()

# --- WebSocket Event Models (for real-time execution updates) ---

class WebSocketEventBase(BaseModel):
    """Base model for all WebSocket events."""
    event_type: str = Field(..., description="Type of the event.")
    timestamp: datetime = Field(default_factory=datetime.utcnow, description="Timestamp of when the event occurred.")
    execution_id: str = Field(..., description="Unique ID for this particular graph execution run.")
    graph_id: str = Field(..., description="ID of the graph definition being executed.")
    
    model_config = ConfigDict(
        alias_generator=to_camel,
        populate_by_name=True,  # Allow both snake_case and camelCase when parsing
        json_encoders={datetime: lambda v: v.isoformat()}
    )

class GraphExecutionStartEvent(WebSocketEventBase):
    event_type: Literal["graph_execution_start"] = "graph_execution_start"
    input_args: Dict[str, Any] = Field(..., description="Initial input arguments provided to the graph.")

class NodeLifecycleEvent(WebSocketEventBase):
    """Base for node-specific events."""
    node_id: str = Field(..., description="ID of the node this event pertains to.")
    node_type: Optional[str] = Field(None, description="Type of the node (e.g., 'llm_node').") # Can be useful for frontend

class NodeStartEvent(NodeLifecycleEvent):
    event_type: Literal["node_start"] = "node_start"
    input_data: Dict[str, Any] = Field(..., description="Data passed as input to this node iteration.")

class NodeEndEvent(NodeLifecycleEvent):
    event_type: Literal["node_end"] = "node_end"
    output_data: Dict[str, Any] = Field(..., description="Data produced as output by this node iteration.")
    status: Literal["success", "failure"] = Field(..., description="Execution status of the node.")
    error_message: Optional[str] = Field(None, description="Error message if the node execution failed.")
    duration_ms: Optional[float] = Field(None, description="Execution duration of the node in milliseconds.")

class EdgeTakenEvent(WebSocketEventBase):
    event_type: Literal["edge_taken"] = "edge_taken"
    source_node_id: str = Field(..., description="ID of the source node of the traversed edge.")
    target_node_id: str = Field(..., description="ID of the target node of the traversed edge.")
    edge_label: Optional[str] = Field(None, description="Label of the edge taken (e.g., condition name).")
    is_conditional: bool = Field(..., description="True if this was a conditional edge.")

class GraphExecutionEndEvent(WebSocketEventBase):
    event_type: Literal["graph_execution_end"] = "graph_execution_end"
    final_state: Dict[str, Any] = Field(..., description="The final state of the graph upon completion.")
    status: Literal["completed", "failed", "interrupted"] = Field(..., description="Overall status of the graph execution.")
    total_duration_ms: Optional[float] = Field(None, description="Total execution duration of the graph in milliseconds.")

class GraphErrorEvent(WebSocketEventBase):
    event_type: Literal["graph_error"] = "graph_error"
    message: str = Field(..., description="General error message related to graph execution.")
    details: Optional[str] = Field(None, description="Additional details or stack trace for the error.")
    node_id: Optional[str] = Field(None, description="ID of the node where the error occurred, if applicable.")

# --- API Request/Response Models (for HTTP endpoints) ---

class GraphDefinitionIdentifier(BaseModel):
    """Minimal information to identify a graph definition."""
    id: str = Field(..., description="Unique ID of the graph definition.")
    name: str = Field(..., description="Name of the graph definition.")
    updated_at: Optional[datetime] = Field(None, description="Last update timestamp.")

    @field_validator('updated_at', mode='before')
    @classmethod
    def parse_datetime(cls, v):
        # Handle None values
        if v is None:
            return None
        # If string, parse it correctly to datetime
        if isinstance(v, str):
            try:
                return datetime.fromisoformat(v.replace('Z', '+00:00'))
            except ValueError:
                try:
                    return datetime.strptime(v, "%Y-%m-%dT%H:%M:%S.%fZ")
                except ValueError:
                    try:
                        return datetime.strptime(v, "%Y-%m-%dT%H:%M:%SZ")
                    except ValueError:
                        return datetime.utcnow()  # Default to current time if parsing fails
        return v

class GraphDefinitionListResponse(BaseModel):
    """Response model for listing available graph definitions."""
    graphs: List[GraphDefinitionIdentifier] = Field(..., description="List of available graph definitions.")

class CreateGraphRequest(BaseModel):
    """Request to create a new graph definition. Most fields from GraphDefinition, ID is generated."""
    name: str = Field(..., description="Human-readable name for the graph.")
    description: Optional[str] = Field(None, description="Optional detailed description of the graph's purpose.")
    state_schema_name: str = Field(..., description="Name of the Pydantic model representing the LangGraph's state.")
    nodes: List[NodeDefinition] = Field(..., description="List of all nodes in the graph.")
    edges: List[EdgeDefinition] = Field(default_factory=list, description="List of all standard (non-conditional) edges.")
    conditional_edges: List[ConditionalEdgesDefinition] = Field(default_factory=list, description="List of conditional edge configurations.")
    entry_point_node_id: str = Field(..., description="ID of the node that serves as the entry point to the graph.")
    terminal_node_ids: Optional[List[str]] = Field(default_factory=list, description="Optional: List of node IDs that should implicitly transition to the global END state.")

    # Example of how you might want to model this if you are providing the full definition on creation,
    # but often ID, created_at, updated_at are server-generated.
    # For this, we'll assume the user provides the core structure and server adds metadata.

class UpdateGraphRequest(CreateGraphRequest): # Inherits fields from CreateGraphRequest
    """Request to update an existing graph definition. Includes all structural fields."""
    # ID will be in the path, name can be updated.
    pass

# GraphDefinition itself can serve as the response for GET /graph/{graph_id}

class ExecuteGraphRequest(BaseModel):
    """Request to initiate execution of a graph."""
    # graph_id will typically be a path parameter
    input_args: Dict[str, Any] = Field(default_factory=dict, description="Initial input arguments for the graph execution.")
    config_overrides: Optional[Dict[str, Any]] = Field(None, description="Optional overrides for graph execution configuration.")
    simulation_delay_ms: Optional[int] = Field(None, ge=0, description="Optional: Simulate delay in ms for each node step.")

class ExecuteGraphResponse(BaseModel):
    """Response after initiating a graph execution (e.g., via HTTP if not solely WebSocket)."""
    execution_id: str = Field(..., description="Unique ID for the initiated graph execution run.")
    message: str = Field("Graph execution started. Follow WebSocket for updates.", description="Status message.")
    websocket_url: Optional[str] = Field(None, description="URL for the WebSocket connection to stream events for this execution.")

# Generic success/error messages
class MessageResponse(BaseModel):
    message: str

class ErrorDetail(BaseModel):
    loc: Optional[List[str | int]] = None
    msg: str
    type: str

class HTTPErrorResponse(BaseModel):
    detail: str | List[ErrorDetail]

File: rag/langgraph_vis/sse_handler.py
import asyncio
import json
import logging
from typing import AsyncGenerator, Dict, Any, Optional
from datetime import datetime
import uuid

from fastapi import APIRouter
from sse_starlette.sse import EventSourceResponse

from .schemas import (
    GraphExecutionStartEvent,
    NodeStartEvent,
    NodeEndEvent,
    GraphExecutionEndEvent,
    GraphErrorEvent,
    ExecuteGraphRequest,
)
from .core.builder import DynamicGraphBuilder, DynamicGraphBuilderError
from .core.definitions import STATIC_GRAPHS_METADATA
from .api_routes import _load_graph_definition_from_file

logger = logging.getLogger(__name__)
try:
    from .ws_handler import CustomJSONEncoder
except ImportError:
    logger.warning("CustomJSONEncoder not found in ws_handler. Using basic json.JSONEncoder.")
    CustomJSONEncoder = json.JSONEncoder

router = APIRouter()


async def event_generator(
    graph_id: str,
    execution_id: str,
    input_args: Dict[str, Any],
    simulation_delay_ms: Optional[int] = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """Generate SSE events for graph execution."""

    def format_sse_data(event_model_instance):
        return json.dumps(event_model_instance.model_dump(mode="json", by_alias=True), cls=CustomJSONEncoder)

    try:
        compiled_graph = None
        if graph_id.startswith("static_"):
            static_graph_name = graph_id[len("static_"):]
            if static_graph_name in STATIC_GRAPHS_METADATA:
                compiled_graph = STATIC_GRAPHS_METADATA[static_graph_name]["compiled_graph"]
            else:
                error_payload_dict = {
                    "eventType": "graph_error", "executionId": execution_id, "graphId": graph_id,
                    "message": f"Static graph '{static_graph_name}' not found.",
                    "timestamp": datetime.utcnow().isoformat()
                }
                yield {"event": "graph_error", "data": json.dumps(error_payload_dict, cls=CustomJSONEncoder)}
                return
        else:
            graph_def = _load_graph_definition_from_file(graph_id)
            if not graph_def:
                error_payload_dict = {
                    "eventType": "graph_error", "executionId": execution_id, "graphId": graph_id,
                    "message": f"Graph definition ID '{graph_id}' not found.",
                    "timestamp": datetime.utcnow().isoformat()
                }
                yield {"event": "graph_error", "data": json.dumps(error_payload_dict, cls=CustomJSONEncoder)}
                return
            try:
                compiled_graph = DynamicGraphBuilder(graph_def).build()
            except DynamicGraphBuilderError as e:
                error_payload_dict = {
                    "eventType": "graph_error", "executionId": execution_id, "graphId": graph_id,
                    "message": f"Failed to build graph: {str(e)}",
                    "timestamp": datetime.utcnow().isoformat()
                }
                yield {"event": "graph_error", "data": json.dumps(error_payload_dict, cls=CustomJSONEncoder)}
                return

        if not compiled_graph:
            error_payload_dict = {
                "eventType": "graph_error", "executionId": execution_id, "graphId": graph_id,
                "message": "Graph could not be compiled or loaded.",
                "timestamp": datetime.utcnow().isoformat()
            }
            yield {"event": "graph_error", "data": json.dumps(error_payload_dict, cls=CustomJSONEncoder)}
            return

        start_event = GraphExecutionStartEvent(
            execution_id=execution_id, graph_id=graph_id, input_args=input_args
        )
        yield {"event": "graph_execution_start", "data": format_sse_data(start_event)}

        execution_config = {"recursion_limit": 25}
        if simulation_delay_ms is not None:
            execution_config["simulation_delay_ms"] = simulation_delay_ms

        async for event_chunk in compiled_graph.astream_events(
            input_args, version="v2", config=execution_config
        ):
            event_type_lc = event_chunk["event"]
            event_data = event_chunk.get("data", {})
            event_name = event_chunk.get("name", "")
            tags = event_chunk.get("tags", [])
            node_id_from_tag = next((tag.split(":")[-1] for tag in tags if tag.startswith("langgraph:node:")), event_name)

            if any(tag.startswith("langgraph:node:") for tag in tags):
                if event_type_lc in ["on_chain_start", "on_tool_start", "on_chat_model_start"]:
                    node_start_event = NodeStartEvent(
                        execution_id=execution_id, graph_id=graph_id, node_id=node_id_from_tag,
                        input_data=event_data.get("input", event_data.get("input_str", {}))
                    )
                    yield {"event": "node_start", "data": format_sse_data(node_start_event)}

                elif event_type_lc in ["on_chain_end", "on_tool_end", "on_chat_model_end"]:
                    status_val = "success"
                    error_msg = None
                    output = event_data.get("output", {})
                    if isinstance(output, Exception):
                        status_val = "failure"; error_msg = str(output)

                    node_end_event = NodeEndEvent(
                        execution_id=execution_id, graph_id=graph_id, node_id=node_id_from_tag,
                        output_data=output if status_val == "success" else {"error": error_msg},
                        status=status_val, error_message=error_msg
                    )
                    yield {"event": "node_end", "data": format_sse_data(node_end_event)}

        final_state_data = event_chunk.get("data", {}).get("output", {}) if 'event_chunk' in locals() and event_chunk else {}
        graph_end_event = GraphExecutionEndEvent(
            execution_id=execution_id, graph_id=graph_id,
            final_state=final_state_data, status="completed"
        )
        yield {"event": "graph_execution_end", "data": format_sse_data(graph_end_event)}

    except Exception as e:
        logger.error(f"Error during SSE stream for execution '{execution_id}': {e}", exc_info=True)
        error_payload_dict = {
            "eventType": "graph_error", "executionId": execution_id, "graphId": graph_id,
            "message": "An unexpected error occurred during graph execution.",
            "details": str(e), "timestamp": datetime.utcnow().isoformat()
        }
        yield {"event": "graph_error", "data": json.dumps(error_payload_dict, cls=CustomJSONEncoder)}


@router.post("/graphs/{graph_id}/execute/stream")
async def execute_graph_sse(
    graph_id: str,
    request_body: ExecuteGraphRequest,
):
    """Execute a graph and stream events via SSE."""
    execution_id = f"sse_exec_{uuid.uuid4().hex[:12]}"
    input_args = request_body.input_args if request_body.input_args is not None else {}
    simulation_delay_ms = request_body.simulation_delay_ms

    logger.info(f"SSE execution request for graph '{graph_id}' with execution_id '{execution_id}'")

    return EventSourceResponse(
        event_generator(graph_id, execution_id, input_args, simulation_delay_ms),
        headers={
            "Cache-Control": "no-cache",
            "X-Execution-ID": execution_id,
        }
    )

File: rag/langgraph_vis/api_routes.py
# rag/langgraph_vis/api_routes.py
import logging
import os
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

from fastapi import APIRouter, HTTPException, Body, Depends, Path as FastAPIPath, Query, WebSocket
from fastapi.responses import JSONResponse

from .schemas import (
    GraphDefinition,
    GraphDefinitionListResponse,
    GraphDefinitionIdentifier,
    CreateGraphRequest,
    UpdateGraphRequest,
    ExecuteGraphRequest, 
    ExecuteGraphResponse, 
    MessageResponse,
    NodeDefinition,
    EdgeDefinition, # Added import
    ConditionalEdgesDefinition # Added import
)
from .core.builder import DynamicGraphBuilder, DynamicGraphBuilderError
# Import STATIC_GRAPHS_METADATA instead of just STATIC_GRAPHS
from .core.definitions import STATIC_GRAPHS, STATIC_GRAPHS_METADATA, STATE_SCHEMAS

from fastapi import Security 
async def get_current_active_user_placeholder():
    class DummyUser:
        username: str = "test_admin"
    return DummyUser()

logger = logging.getLogger(__name__)
router = APIRouter()

GRAPH_DEFINITIONS_DIR = Path(os.getenv("GRAPH_DEFINITIONS_DIR", "data/graph_definitions"))
GRAPH_DEFINITIONS_DIR.mkdir(parents=True, exist_ok=True)
logger.info(f"Graph definitions will be stored in: {GRAPH_DEFINITIONS_DIR.resolve()}")

def _get_graph_definition_path(graph_id: str) -> Path:
    if ".." in graph_id or "/" in graph_id or "\\" in graph_id:
        raise ValueError("Invalid characters in graph_id.")
    return GRAPH_DEFINITIONS_DIR / f"{graph_id}.json"

def _load_graph_definition_from_file(graph_id: str) -> GraphDefinition | None:
    file_path = _get_graph_definition_path(graph_id)
    if file_path.exists():
        try:
            with open(file_path, "r") as f:
                data = json.load(f)
            return GraphDefinition(**data)
        except json.JSONDecodeError:
            logger.error(f"Error decoding JSON for graph ID '{graph_id}' from {file_path}")
            return None
        except Exception as e: 
            logger.error(f"Error loading/validating graph definition for graph ID '{graph_id}': {e}")
            return None
    return None

def _save_graph_definition_to_file(graph_def: GraphDefinition) -> None:
    file_path = _get_graph_definition_path(graph_def.id)
    try:
        with open(file_path, "w") as f:
            json.dump(graph_def.model_dump(mode="json"), f, indent=2)
        logger.info(f"Saved graph definition '{graph_def.name}' (ID: {graph_def.id}) to {file_path}")
    except Exception as e:
        logger.error(f"Error saving graph definition ID '{graph_def.id}' to {file_path}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save graph definition: {str(e)}")

def _delete_graph_definition_file(graph_id: str) -> bool:
    file_path = _get_graph_definition_path(graph_id)
    if file_path.exists():
        try:
            file_path.unlink()
            logger.info(f"Deleted graph definition file for ID '{graph_id}': {file_path}")
            return True
        except Exception as e:
            logger.error(f"Error deleting graph definition file for ID '{graph_id}': {e}")
            return False
    return False 

@router.post(
    "/graphs",
    response_model=GraphDefinition,
    status_code=201,
    summary="Create a new graph definition",
    dependencies=[Depends(get_current_active_user_placeholder)] 
)
async def create_graph_definition(
    graph_create_request: CreateGraphRequest = Body(...)
):
    logger.info(f"Received request to create graph: {graph_create_request.name}")
    graph_def = GraphDefinition(**graph_create_request.model_dump())
    try:
        DynamicGraphBuilder(graph_def).build() 
        logger.info(f"Successfully test-built new graph definition: {graph_def.name}")
    except DynamicGraphBuilderError as e:
        logger.error(f"Validation error during test-build for new graph '{graph_def.name}': {e}")
        raise HTTPException(status_code=422, detail=f"Invalid graph structure: {str(e)}")
    _save_graph_definition_to_file(graph_def)
    return graph_def

@router.get(
    "/graphs",
    response_model=GraphDefinitionListResponse,
    summary="List all saved graph definitions"
)
async def list_graph_definitions(
    include_static: bool = Query(False, description="Include predefined static graphs in the listing")
):
    saved_graphs: List[GraphDefinitionIdentifier] = []
    for file_path in GRAPH_DEFINITIONS_DIR.glob("*.json"):
        graph_id = file_path.stem
        graph_def = _load_graph_definition_from_file(graph_id)
        if graph_def:
            saved_graphs.append(
                GraphDefinitionIdentifier(id=graph_def.id, name=graph_def.name, updated_at=graph_def.updated_at)
            )
    
    if include_static:
        for name, meta_data in STATIC_GRAPHS_METADATA.items():
            static_graph_id = f"static_{name}"
            if not any(g.id == static_graph_id for g in saved_graphs):
                compiled_graph = meta_data["compiled_graph"]
                updated_at_static = compiled_graph.compiled_at if hasattr(compiled_graph, 'compiled_at') else datetime.utcnow()
                saved_graphs.append(
                    GraphDefinitionIdentifier(
                        id=static_graph_id,
                        name=f"[STATIC] {name.replace('_', ' ').title()}",
                        updated_at=updated_at_static
                    )
                )
    return GraphDefinitionListResponse(graphs=saved_graphs)

@router.get(
    "/graphs/{graph_id}",
    response_model=GraphDefinition, 
    summary="Get a specific graph definition"
)
async def get_graph_definition(
    graph_id: str = FastAPIPath(..., description="The ID of the graph definition to retrieve.")
):
    logger.debug(f"Request to get graph definition for ID: {graph_id}")
    
    if graph_id.startswith("static_"):
        static_graph_name = graph_id[len("static_"):]
        if static_graph_name in STATIC_GRAPHS_METADATA:
            logger.info(f"Constructing GraphDefinition for static graph: {static_graph_name}")
            metadata = STATIC_GRAPHS_METADATA[static_graph_name]
            compiled_graph = metadata["compiled_graph"] # Keep for compiled_at if needed

            # Use the predefined structure from metadata
            return GraphDefinition(
                id=graph_id,
                name=f"[STATIC] {static_graph_name.replace('_', ' ').title()}",
                description=metadata.get("description", f"Static graph: {static_graph_name}"),
                state_schema_name=metadata["state_schema_name"],
                nodes=metadata.get("nodes", []), # Use predefined nodes
                edges=metadata.get("edges", []), # Use predefined edges
                conditional_edges=metadata.get("conditional_edges", []), # Use predefined conditional_edges
                entry_point_node_id=metadata["entry_point_node_id"],
                terminal_node_ids=metadata.get("terminal_node_ids", []), # Use predefined terminal_node_ids
                created_at=compiled_graph.compiled_at if hasattr(compiled_graph, 'compiled_at') else datetime.utcnow(),
                updated_at=compiled_graph.compiled_at if hasattr(compiled_graph, 'compiled_at') else datetime.utcnow(),
                version=1 # Default version for static
            )
        else:
            raise HTTPException(status_code=404, detail=f"Static graph with name '{static_graph_name}' not found.")

    graph_def = _load_graph_definition_from_file(graph_id)
    if not graph_def:
        raise HTTPException(status_code=404, detail=f"Graph definition with ID '{graph_id}' not found.")
    return graph_def

@router.put(
    "/graphs/{graph_id}",
    response_model=GraphDefinition,
    summary="Update an existing graph definition",
    dependencies=[Depends(get_current_active_user_placeholder)] 
)
async def update_graph_definition(
    graph_id: str = FastAPIPath(..., description="The ID of the graph definition to update."),
    graph_update_request: UpdateGraphRequest = Body(...)
):
    logger.info(f"Received request to update graph ID: {graph_id}")
    existing_graph_def = _load_graph_definition_from_file(graph_id)
    if not existing_graph_def:
        raise HTTPException(status_code=404, detail=f"Graph definition with ID '{graph_id}' not found for update.")

    updated_graph_data = graph_update_request.model_dump()
    updated_graph_data["id"] = existing_graph_def.id 
    updated_graph_data["created_at"] = existing_graph_def.created_at 
    updated_graph_data["updated_at"] = datetime.utcnow() 

    updated_graph_def = GraphDefinition(**updated_graph_data)
    # updated_graph_def.updated_at = updated_graph_def.model_fields['updated_at'].default_factory() # Pydantic v2 style

    try:
        DynamicGraphBuilder(updated_graph_def).build()
        logger.info(f"Successfully test-built updated graph definition: {updated_graph_def.name}")
    except DynamicGraphBuilderError as e:
        logger.error(f"Validation error during test-build for updated graph '{updated_graph_def.name}': {e}")
        raise HTTPException(status_code=422, detail=f"Invalid graph structure: {str(e)}")

    _save_graph_definition_to_file(updated_graph_def)
    return updated_graph_def

@router.delete(
    "/graphs/{graph_id}",
    response_model=MessageResponse,
    status_code=200, 
    summary="Delete a graph definition",
    dependencies=[Depends(get_current_active_user_placeholder)]
)
async def delete_graph_definition(
    graph_id: str = FastAPIPath(..., description="The ID of the graph definition to delete.")
):
    logger.info(f"Request to delete graph definition ID: {graph_id}")
    if graph_id.startswith("static_"):
        raise HTTPException(status_code=400, detail="Static graph definitions cannot be deleted via this API.")

    if not _delete_graph_definition_file(graph_id):
        if _get_graph_definition_path(graph_id).exists(): 
             raise HTTPException(status_code=500, detail=f"Failed to delete graph definition file for ID '{graph_id}'.")
        raise HTTPException(status_code=404, detail=f"Graph definition with ID '{graph_id}' not found.")
    
    return MessageResponse(message=f"Graph definition with ID '{graph_id}' deleted successfully.")

@router.post(
    "/graphs/{graph_id}/execute",
    response_model=ExecuteGraphResponse,
    summary="Trigger execution of a graph (non-streaming)",
    dependencies=[Depends(get_current_active_user_placeholder)] 
)
async def execute_graph_http(
    graph_id: str = FastAPIPath(..., description="ID of the graph to execute (saved or static)."),
    fastapi_req: ExecuteGraphRequest = Body(...), # Renamed to avoid conflict with Starlette Request
):
    logger.info(f"HTTP request to execute graph ID: {graph_id} with inputs: {fastapi_req.input_args}")
    
    compiled_graph = None
    if graph_id.startswith("static_"):
        static_graph_name = graph_id[len("static_"):]
        if static_graph_name in STATIC_GRAPHS_METADATA:
            compiled_graph = STATIC_GRAPHS_METADATA[static_graph_name]["compiled_graph"]
        else:
            raise HTTPException(status_code=404, detail=f"Static graph '{static_graph_name}' not found.")
    else:
        graph_def = _load_graph_definition_from_file(graph_id)
        if not graph_def:
            raise HTTPException(status_code=404, detail=f"Graph definition ID '{graph_id}' not found.")
        try:
            compiled_graph = DynamicGraphBuilder(graph_def).build()
        except DynamicGraphBuilderError as e:
            raise HTTPException(status_code=500, detail=f"Failed to build graph '{graph_id}': {str(e)}")

    if not compiled_graph: 
        raise HTTPException(status_code=500, detail=f"Could not load or build graph '{graph_id}'.")

    execution_id = f"exec_{os.urandom(8).hex()}" 
    
    # This path is relative to the LG_VIS_PREFIX defined in app.py
    # If LG_VIS_PREFIX = "/v1/lg-vis", then this will be correct.
    ws_path = router.url_path_for("websocket_execute_graph_with_id", graph_id=graph_id, execution_id=execution_id)
    # The full URL will depend on the request's base URL, which FastAPI can't know directly here easily.
    # Client usually constructs this from relative path or a configured base WS URL.
    # For now, just returning the path.

    logger.info(f"Generated execution ID '{execution_id}' for graph '{graph_id}'. Client should connect to WebSocket at path: {ws_path}")

    return ExecuteGraphResponse(
        execution_id=execution_id,
        message=f"Graph execution '{execution_id}' initiated for graph '{graph_id}'. Connect to WebSocket for updates.",
        websocket_url=str(ws_path) # Return the path
    )

@router.websocket("/ws/langgraph/graphs/{graph_id}/execute")
async def websocket_execute_graph_with_id(
    websocket: WebSocket, # Added WebSocket
    graph_id: str = FastAPIPath(..., description="ID of the graph to execute (saved or static)."),
    execution_id: str = Query(..., description="A unique ID for this execution instance, provided by the client or a preliminary setup call.")
):
    # Placeholder for WebSocket connection and execution logic
    await websocket.accept()
    logger.info(f"WebSocket connection established for graph_id: {graph_id}, execution_id: {execution_id}")
    # Actual graph execution and message streaming would go here
    # For now, just sending a confirmation and closing
    await websocket.send_json({"message": f"WebSocket connected for graph {graph_id}, execution {execution_id}"})
    await websocket.close()

@router.post("/graphs/{graph_id}/execute/sse")
async def execute_graph_sse(
    graph_id: str = FastAPIPath(..., description="ID of the graph to execute (saved or static)."),
    fastapi_req: ExecuteGraphRequest = Body(...), # Renamed to avoid conflict
):
    logger.info(f"SSE request to execute graph ID: {graph_id} with inputs: {fastapi_req.input_args}")
    # Placeholder for SSE graph execution logic
    # Actual graph execution and event streaming would go here
    # This would typically return an EventSourceResponse
    return JSONResponse(
        content={"message": f"SSE execution initiated for graph {graph_id}. Events will be streamed."},
        status_code=200
    )

@router.post("/graphs/{graph_id}/execute/stream")
async def execute_graph_http_stream(
    graph_id: str = FastAPIPath(..., description="ID of the graph to execute (saved or static)."),
    fastapi_req: ExecuteGraphRequest = Body(...), # Renamed to avoid conflict
):
    logger.info(f"HTTP Streaming request to execute graph ID: {graph_id} with inputs: {fastapi_req.input_args}")
    # Placeholder for HTTP Streaming graph execution logic
    # Actual graph execution and chunked response streaming would go here
    # This would typically return a StreamingResponse
    return JSONResponse(
        content={"message": f"HTTP Streaming execution initiated for graph {graph_id}. Data will be streamed."},
        status_code=200
    )

File: rag/langgraph_vis/__init__.py
# rag/langgraph_vis/__init__.py
# This file makes 'langgraph_vis' a Python package.

# You can optionally make key components directly importable from this package level, for example:
# from .api_routes import router as api_router
# from .ws_handler import router as websocket_router
# from .schemas import GraphDefinition
# from .core.builder import DynamicGraphBuilder
# from .core.definitions import NODE_IMPLEMENTATIONS, ROUTER_IMPLEMENTATIONS, STATE_SCHEMAS

# For now, keeping it simple. Specific modules will be imported directly where needed.

File: rag/langgraph_vis/ws_handler.py
# rag/langgraph_vis/ws_handler.py
import logging
import asyncio
import json
import uuid
from typing import Dict, Any, Optional, List, Literal, Type, get_type_hints
import inspect

from fastapi import APIRouter, WebSocket, WebSocketDisconnect, HTTPException, Depends, Query
from starlette.websockets import WebSocketState # For checking state
from pydantic import BaseModel, Field
# Import langchain message types for custom JSON serialization
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage

# Schemas for WebSocket events and potentially initial messages
from .schemas import (
    WebSocketEventBase, # For a common structure, though specific events are better
    GraphExecutionStartEvent,
    NodeStartEvent,
    NodeEndEvent,
    EdgeTakenEvent,
    GraphExecutionEndEvent,
    GraphErrorEvent,
    ExecuteGraphRequest as WebSocketInitialMessage, # Use ExecuteGraphRequest for initial message
)

# Graph building and loading
from .core.builder import DynamicGraphBuilder, DynamicGraphBuilderError
from .core.definitions import STATIC_GRAPHS, STATE_SCHEMAS, STATIC_GRAPHS_METADATA # For type hints and validation
# For loading graph definitions from files
from .api_routes import _load_graph_definition_from_file # Re-use helper from api_routes

logger = logging.getLogger(__name__)
router = APIRouter() # Can use a router or define directly on FastAPI app instance

# Custom JSON encoder to handle AIMessage and other LangChain objects
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj: Any) -> Any:
        # Handle all BaseMessage objects (AIMessage, HumanMessage, etc.)
        if isinstance(obj, BaseMessage):
            return {
                "type": obj.__class__.__name__,
                "role": getattr(obj, "role", "assistant"),
                "content": obj.content,
                "additional_kwargs": obj.additional_kwargs
            }
        
        # Handle other LangChain objects with to_dict() method
        if hasattr(obj, "to_dict") and callable(getattr(obj, "to_dict")):
            return obj.to_dict()
            
        # Handle objects with __dict__ attribute
        if hasattr(obj, "__dict__"):
            return obj.__dict__
            
        # Try string representation as a last resort for other unserializable objects
        try:
            return str(obj)
        except:
            # Call the base class implementation for other types
            return super().default(obj)

# Helper to get a compiled graph (static or dynamic)
async def get_compiled_graph_for_execution(graph_id: str):
    """
    Loads and compiles a graph, either static or from a dynamic definition file.
    """
    if graph_id.startswith("static_"):
        static_graph_name = graph_id[len("static_"):]
        if static_graph_name in STATIC_GRAPHS:
            return STATIC_GRAPHS[static_graph_name]
        else:
            logger.warning(f"Static graph '{static_graph_name}' not found for execution.")
            raise DynamicGraphBuilderError(f"Static graph '{static_graph_name}' not found.")
    else:
        graph_def = _load_graph_definition_from_file(graph_id)
        if not graph_def:
            logger.warning(f"Graph definition ID '{graph_id}' not found for execution.")
            raise DynamicGraphBuilderError(f"Graph definition ID '{graph_id}' not found.")
        try:
            # Build the graph in a separate thread to avoid blocking the event loop if it's CPU-intensive
            # For very complex graphs, consider a process pool or task queue.
            loop = asyncio.get_running_loop()
            builder = DynamicGraphBuilder(graph_def)
            compiled_graph = await loop.run_in_executor(None, builder.build) # builder.build is synchronous
            return compiled_graph
        except DynamicGraphBuilderError as e:
            logger.error(f"Failed to build dynamic graph '{graph_id}': {e}", exc_info=True)
            raise
        except Exception as e: # Catch any other unexpected errors during build
            logger.error(f"Unexpected error building dynamic graph '{graph_id}': {e}", exc_info=True)
            raise DynamicGraphBuilderError(f"Unexpected error building graph: {str(e)}")


@router.websocket("/ws/langgraph/graphs/{graph_id}/execute") # Path matches placeholder in api_routes
async def websocket_execute_graph_legacy_path(
    websocket: WebSocket,
    graph_id: str,
    # Optional: execution_id could be passed by a client if pre-generated by HTTP trigger
    # execution_id: Optional[str] = Query(None, alias="execId"),
    # Optional: client_id for tracking connections
    # client_id: Optional[str] = Query(None)
):
    """
    WebSocket endpoint to execute a LangGraph and stream events.
    This path does not include execution_id; a new one will be generated.
    """
    # Generate a unique execution ID for this run
    current_execution_id = f"ws_exec_{uuid.uuid4().hex[:12]}"
    logger.info(f"WebSocket connection request for graph_id='{graph_id}', new execution_id='{current_execution_id}'")
    await _handle_graph_execution_websocket(websocket, graph_id, current_execution_id)

@router.websocket("/ws/langgraph/graphs/{graph_id}/execute/{execution_id}") # Path matches placeholder in api_routes
async def websocket_execute_graph_with_id(
    websocket: WebSocket,
    graph_id: str,
    execution_id: str,
    # Optional: client_id for tracking connections
    # client_id: Optional[str] = Query(None)
):
    """
    WebSocket endpoint to execute a LangGraph and stream events,
    using a pre-defined execution_id (e.g., from an HTTP trigger).
    """
    logger.info(f"WebSocket connection request for graph_id='{graph_id}', using execution_id='{execution_id}'")
    await _handle_graph_execution_websocket(websocket, graph_id, execution_id)


async def _handle_graph_execution_websocket(
    websocket: WebSocket,
    graph_id: str,
    execution_id: str
):
    """
    Core logic for handling WebSocket graph execution.
    """
    client_host = websocket.client.host if websocket.client else "unknown"
    client_port = websocket.client.port if websocket.client else "unknown"
    logger.info(f"WebSocket connection accepted for graph_id='{graph_id}', execution_id='{execution_id}' from {client_host}:{client_port}")

    await websocket.accept()

    compiled_graph = None
    initial_input_args: Dict[str, Any] = {}

    try:
        # 1. Load or Build the Graph
        try:
            compiled_graph = await get_compiled_graph_for_execution(graph_id)
            logger.info(f"Successfully loaded/built graph '{graph_id}' for execution_id '{execution_id}'.")
        except DynamicGraphBuilderError as e:
            logger.error(f"Failed to load/build graph '{graph_id}' for exec_id '{execution_id}': {e}")
            error_event = GraphErrorEvent(
                execution_id=execution_id, graph_id=graph_id, message=f"Graph loading/building error: {str(e)}"
            )
            # Use custom JSON encoder to handle any special objects
            json_data = json.dumps({
                "eventType": error_event.event_type,
                "timestamp": error_event.timestamp.isoformat(),
                "executionId": error_event.execution_id,
                "graphId": error_event.graph_id,
                "message": error_event.message,
                "details": error_event.details
            }, cls=CustomJSONEncoder)
            await websocket.send_text(json_data)
            await websocket.close(code=1011) # Internal error
            return
        except Exception as e: # Catch any other unexpected errors
            logger.error(f"Unexpected error preparing graph '{graph_id}' for exec_id '{execution_id}': {e}", exc_info=True)
            error_event = GraphErrorEvent(
                execution_id=execution_id, graph_id=graph_id, message=f"Unexpected server error preparing graph: {str(e)}"
            )
            # Use custom JSON encoder to handle any special objects
            json_data = json.dumps({
                "eventType": error_event.event_type,
                "timestamp": error_event.timestamp.isoformat(),
                "executionId": error_event.execution_id,
                "graphId": error_event.graph_id,
                "message": error_event.message,
                "details": error_event.details
            }, cls=CustomJSONEncoder)
            await websocket.send_text(json_data)
            await websocket.close(code=1011)
            return
            
        # 2. Wait for initial message from client
        try:
            # Wait for and parse the initial message using our Pydantic model
            initial_message_raw = await asyncio.wait_for(websocket.receive_text(), timeout=10.0)
            initial_message_data_raw = json.loads(initial_message_raw)
            # Validate with Pydantic model
            initial_message = WebSocketInitialMessage(**initial_message_data_raw)
            
            initial_input_args = initial_message.input_args
            config_overrides = {}
            if initial_message.config_overrides:
                config_overrides = initial_message.config_overrides # Store for later use
            simulation_delay_ms = None
            if initial_message.simulation_delay_ms is not None:
                simulation_delay_ms = initial_message.simulation_delay_ms
            
            logger.info(f"Received initial input for exec_id '{execution_id}': args={initial_input_args}, delay_ms={simulation_delay_ms}")
        except asyncio.TimeoutError:
            logger.info(f"No initial input message received for exec_id '{execution_id}'. Using defaults.")
        except (json.JSONDecodeError, Exception) as e: # Catch Pydantic validation errors too
            logger.warning(f"Invalid initial message for exec_id '{execution_id}': {e}. Using defaults.")
        except asyncio.TimeoutError:
            logger.info(f"No initial input message received within timeout for exec_id '{execution_id}'. Proceeding with empty inputs.")
        except json.JSONDecodeError:
            logger.warning(f"Invalid JSON in initial message for exec_id '{execution_id}'. Proceeding with empty inputs.")
        except WebSocketDisconnect: # Client disconnected before sending initial message
            logger.info(f"Client disconnected before sending initial input for exec_id '{execution_id}'.")
            return # Gracefully exit
        except Exception as e:
            logger.error(f"Error processing initial message for exec_id '{execution_id}': {e}", exc_info=True)        # Proceed with empty inputs, or send an error and close
        
        # Determine the graph's state schema type and provide default values for required fields
        state_schema_name = None
        state_schema_type = None
        
        # Special case for DocumentProcessingState - directly check if this is example_document_workflow
        if graph_id == "static_example_document_workflow" or graph_id.endswith("example_document_workflow"):
            # Ensure required original_document field is always present
            if "original_document" not in initial_input_args or not initial_input_args["original_document"]:
                initial_input_args["original_document"] = "Default document for visualization"
                logger.info(f"Added required 'original_document' field for DocumentProcessingState in graph '{graph_id}'")
        
        # Try to get state schema for static graphs from metadata
        if graph_id.startswith("static_"):
            static_graph_name = graph_id[len("static_"):]
            if static_graph_name in STATIC_GRAPHS_METADATA:
                state_schema_name = STATIC_GRAPHS_METADATA[static_graph_name].get("state_schema_name")
                if state_schema_name and state_schema_name in STATE_SCHEMAS:
                    state_schema_type = STATE_SCHEMAS[state_schema_name]
                    logger.info(f"Found state schema '{state_schema_name}' for graph '{graph_id}'")
        
        # Add required fields based on schema name
        if state_schema_name == "DocumentProcessingState":
            # Always ensure original_document is present for DocumentProcessingState
            if "original_document" not in initial_input_args or not initial_input_args["original_document"]:
                initial_input_args["original_document"] = "Default document for visualization"
                logger.info(f"Added required 'original_document' field for DocumentProcessingState")
        elif state_schema_name == "BasicAgentState":
            # For BasicAgentState, ensure messages field is present
            if "messages" not in initial_input_args:
                initial_input_args["messages"] = []
                logger.info(f"Added required 'messages' field for BasicAgentState")
        
        # For all schema types, try validation and error correction
        if state_schema_type:
            try:
                # For Pydantic models, try to validate with the model
                if issubclass(state_schema_type, BaseModel):
                    model_instance = state_schema_type(**initial_input_args)
                    logger.info(f"Successfully validated input args against {state_schema_name} schema")
            except Exception as validation_error:
                error_msg = str(validation_error)
                logger.warning(f"Validation error for {state_schema_name}: {validation_error}")
                
                # Extract field names from error message
                import re
                field_errors = re.findall(r"field required for (\w+)", error_msg.lower())
                
                for field in field_errors:
                    if field == "original_document":
                        initial_input_args[field] = "Default document added after validation error"
                    elif field == "messages":
                        initial_input_args[field] = []
                    else:
                        # Default to empty string for unknown fields
                        initial_input_args[field] = ""
                    logger.info(f"Added missing required field '{field}' based on validation error")
        
        logger.info(f"Input args after adding defaults: {initial_input_args}")

        # 3. Stream Graph Execution Events
        start_event = GraphExecutionStartEvent(
            execution_id=execution_id, graph_id=graph_id, input_args=initial_input_args
        )
        # Use custom JSON encoder to handle any special objects in input args
        json_data = json.dumps({
            "eventType": start_event.event_type,
            "timestamp": start_event.timestamp.isoformat(),
            "executionId": start_event.execution_id,
            "graphId": start_event.graph_id,
            "inputArgs": start_event.input_args
        }, cls=CustomJSONEncoder)
        await websocket.send_text(json_data)

        # Recursion limit: Important for LangGraph
        recursion_limit = 25 
        
        # Prepare config for LangGraph execution
        execution_config = {"recursion_limit": recursion_limit}
        if config_overrides: # Merge any explicit config_overrides from client
            execution_config.update(config_overrides)
        
        # THIS IS WHERE YOU PASS simulation_delay_ms TO THE GRAPH'S CONTEXT/CONFIG
        if simulation_delay_ms is not None:
            # Since nodes need to check for this value, let's add it to the execution config
            # Option 1: Pass it through the execution_config so node implementations can access it
            execution_config["simulation_delay_ms"] = simulation_delay_ms
            logger.info(f"Execution will use simulation_delay_ms: {simulation_delay_ms} (nodes must be adapted to use it from their config)")

        async for event_chunk in compiled_graph.astream_events(
            initial_input_args, 
            version="v2", 
            config=execution_config # Pass the prepared config
        ):
            event_type = event_chunk["event"]
            event_data = event_chunk.get("data", {})
            event_name = event_chunk.get("name", "") # Often the node name or runnable name
            tags = event_chunk.get("tags", [])

            # Ensure websocket is still open before sending
            if websocket.client_state != WebSocketState.CONNECTED:
                logger.warning(f"WebSocket disconnected during streaming for exec_id '{execution_id}'. Stopping stream.")
                break

            # --- Map LangGraph events to our defined WebSocket schemas ---
            # This mapping might need adjustment based on the verbosity and structure of LangGraph's `astream_events`
            # For "v2" events:
            # - "on_chain_start", "on_chain_end", "on_chain_stream"
            # - "on_llm_start", "on_llm_end", "on_llm_stream"
            # - "on_tool_start", "on_tool_end"
            # - "on_chat_model_start", ...
            # - tags: ["langgraph:node:<node_name>"], ["langgraph:edge"], ["langgraph:action"] etc.

            if "langgraph:node:"+event_name in tags: # Heuristic for identifying node-related events by tag
                if event_type == "on_chain_start" or event_type == "on_tool_start" or event_type == "on_chat_model_start": # General start events for nodes
                    node_start_ws_event = NodeStartEvent(
                        execution_id=execution_id,
                        graph_id=graph_id,
                        node_id=event_name, # `name` field from event usually corresponds to node ID
                        input_data=event_data.get("input", event_data.get("input_str", {})) # Prefer "input" if available
                    )
                    # Use custom JSON encoder to handle AIMessage objects
                    json_data = json.dumps({
                        "eventType": node_start_ws_event.event_type,
                        "timestamp": node_start_ws_event.timestamp.isoformat(),
                        "executionId": node_start_ws_event.execution_id,
                        "graphId": node_start_ws_event.graph_id,
                        "nodeId": node_start_ws_event.node_id,
                        "nodeType": node_start_ws_event.node_type,
                        "inputData": node_start_ws_event.input_data
                    }, cls=CustomJSONEncoder)
                    await websocket.send_text(json_data)

                elif event_type == "on_chain_end" or event_type == "on_tool_end" or event_type == "on_chat_model_end": # General end events for nodes
                    status: Literal["success", "failure"] = "success" # type: ignore
                    error_msg: Optional[str] = None
                    # LangGraph event structure for errors might vary. Check if output is an exception.
                    output = event_data.get("output", {})
                    if isinstance(output, Exception):
                        status = "failure"
                        error_msg = str(output)
                    elif isinstance(output, dict) and output.get("messages") and isinstance(output["messages"][-1], BaseException): # type: ignore
                        status = "failure"
                        error_msg = str(output["messages"][-1])

                    node_end_ws_event = NodeEndEvent(
                        execution_id=execution_id,
                        graph_id=graph_id,
                        node_id=event_name,
                        output_data=output if status == "success" else {"error": error_msg},
                        status=status,
                        error_message=error_msg
                        # duration_ms: event_data.get("duration_ms") # If LangGraph provides this directly
                    )
                    # Use custom JSON encoder to handle AIMessage objects
                    json_data = json.dumps({
                        "eventType": node_end_ws_event.event_type,
                        "timestamp": node_end_ws_event.timestamp.isoformat(),
                        "executionId": node_end_ws_event.execution_id,
                        "graphId": node_end_ws_event.graph_id,
                        "nodeId": node_end_ws_event.node_id,
                        "nodeType": node_end_ws_event.node_type,
                        "outputData": node_end_ws_event.output_data,
                        "status": node_end_ws_event.status,
                        "errorMessage": node_end_ws_event.error_message,
                        "durationMs": node_end_ws_event.duration_ms
                    }, cls=CustomJSONEncoder)
                    await websocket.send_text(json_data)

            # Check for edge events (experimental based on potential tag)
            # Actual edge traversal might need to be inferred or LangGraph might offer more direct events.
            # LangGraph's internal "implicit" edges (like from a node to END) might not produce an "edge" event.
            # This part is highly dependent on LangGraph's `astream_events` v2 structure for edges.
            if "langgraph:edge" in tags and event_type == "on_chain_start": # Assuming edge leads to start of next chain
                # This is a guess. `astream_events` `data` for an edge event might contain source/target.
                # We might need to maintain previous node to infer source.
                # For now, let's assume the `name` for an edge event could be 'source_id:target_id:condition'
                # Or, it might be that `event_data` gives source/target explicitly for edge events.
                # This part needs careful testing with actual `astream_events` v2 output for edges.
                # Simpler approach: infer edge from node_end -> next_node_start.
                pass # Placeholder for more robust edge event handling

            # Placeholder for sending EdgeTakenEvent (often inferred from node sequence if not explicit)

        # 4. Send Graph Execution End event
        # The final state is usually the last event_chunk's data if it's the graph's end.
        # However, `astream_events` itself doesn't give a single "final state" object directly.
        # The last relevant "on_chain_end" for the graph itself or END node would be key.
        # For now, we'll make a best guess or acknowledge completion.
        # A more robust way is to capture the final output from `ainvoke`. Here, we're streaming.
        final_state_data = event_chunk.get("data", {}).get("output", {}) if 'event_chunk' in locals() else {} # type: ignore
        graph_end_event = GraphExecutionEndEvent(
            execution_id=execution_id,
            graph_id=graph_id,
            final_state=final_state_data, # This is an approximation from the stream
            status="completed" # Assuming successful completion if loop finishes
        )
        # Use custom JSON encoder to handle AIMessage objects
        json_data = json.dumps({
            "eventType": graph_end_event.event_type,
            "timestamp": graph_end_event.timestamp.isoformat(),
            "executionId": graph_end_event.execution_id,
            "graphId": graph_end_event.graph_id,
            "finalState": graph_end_event.final_state,
            "status": graph_end_event.status,
            "totalDurationMs": graph_end_event.total_duration_ms
        }, cls=CustomJSONEncoder)
        await websocket.send_text(json_data)
        logger.info(f"Graph execution stream completed for exec_id '{execution_id}'.")

    except WebSocketDisconnect:
        logger.info(f"Client {client_host}:{client_port} (exec_id '{execution_id}') disconnected.")
    except DynamicGraphBuilderError as e: # Catch errors from get_compiled_graph_for_execution if not caught earlier
        logger.error(f"Graph Builder Error for exec_id '{execution_id}', graph_id '{graph_id}': {e}")
        if websocket.client_state == WebSocketState.CONNECTED:
            error_event = GraphErrorEvent(execution_id=execution_id, graph_id=graph_id, message=str(e))
            # Use custom JSON encoder to handle any special objects
            json_data = json.dumps({
                "eventType": error_event.event_type,
                "timestamp": error_event.timestamp.isoformat(),
                "executionId": error_event.execution_id,
                "graphId": error_event.graph_id,
                "message": error_event.message,
                "details": error_event.details
            }, cls=CustomJSONEncoder)
            await websocket.send_text(json_data)
    except Exception as e:
        logger.error(f"Unhandled error during WebSocket execution for exec_id '{execution_id}': {e}", exc_info=True)
        if websocket.client_state == WebSocketState.CONNECTED:
            error_event = GraphErrorEvent(
                execution_id=execution_id,
                graph_id=graph_id,
                message="An unexpected server error occurred during graph execution.",
                details=str(e)
            )
            # Use custom JSON encoder to handle any special objects
            json_data = json.dumps({
                "eventType": error_event.event_type,
                "timestamp": error_event.timestamp.isoformat(),
                "executionId": error_event.execution_id,
                "graphId": error_event.graph_id,
                "message": error_event.message,
                "details": error_event.details
            }, cls=CustomJSONEncoder)
            await websocket.send_text(json_data)
    finally:
        if websocket.client_state == WebSocketState.CONNECTED:
            await websocket.close()
        logger.info(f"WebSocket connection closed for exec_id '{execution_id}' from {client_host}:{client_port}")

# How to integrate this router into your main FastAPI app (e.g., in rag/api/app.py):
# from rag.langgraph_vis import ws_handler # Assuming this structure
# app.include_router(ws_handler.router)
# Or, if not using a router, you'd define the @app.websocket directly in your main app file
# and call _handle_graph_execution_websocket.


File: rag/langgraph_vis/core/__init__.py
# rag/langgraph_vis/core/__init__.py
# This file makes 'core' a Python sub-package within 'langgraph_vis'.

# You can make core components directly importable:
# from .definitions import (
#     NODE_IMPLEMENTATIONS,
#     ROUTER_IMPLEMENTATIONS,
#     STATE_SCHEMAS,
#     STATIC_GRAPHS,
#     BasicAgentState, # Example state
#     DocumentProcessingState # Example state
# )
# from .builder import DynamicGraphBuilder, DynamicGraphBuilderError

# For now, keeping it simple.

File: rag/langgraph_vis/core/definitions.py
# rag/langgraph_vis/core/definitions.py
import logging
from typing import TypedDict, Annotated, List, Optional, Dict, Any
import operator
import asyncio 
from datetime import datetime # Ensure datetime is imported

from pydantic import BaseModel, Field
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage

from langgraph.graph import StateGraph, END
from langgraph.graph.graph import CompiledGraph

# Import schemas needed for defining the structure in metadata
from ..schemas import NodeDefinition as SchemaNodeDefinition, EdgeDefinition as SchemaEdgeDefinition, ConditionalEdgesDefinition as SchemaConditionalEdgesDefinition, ConditionalEdgeMapping as SchemaConditionalEdgeMapping, NodeUIPosition

logger = logging.getLogger(__name__)

# --- 1. Pydantic State Definitions ---
class BasicAgentState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    next_node: Optional[str] = None
    last_tool_call: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None

class DocumentProcessingState(BaseModel):
    original_document: str = Field(..., description="The initial document text.")
    processed_document: Optional[str] = Field(None, description="Document after some processing step.")
    summary: Optional[str] = Field(None, description="Summary of the document.")
    keywords: List[str] = Field(default_factory=list, description="Extracted keywords.")
    confidence_score: Optional[float] = Field(None, description="A score related to some processing step.")
    is_processed_successfully: bool = False
    error_info: Optional[str] = None
    class Config:
        extra = "allow"

# --- Helper function for simulation delay ---
async def apply_simulation_delay(config: Optional[Dict[str, Any]], default_delay_s: float = 0):
    """Applies a delay if simulation_delay_ms is found in config."""
    if config and "simulation_delay_ms" in config:
        delay_ms = config.get("simulation_delay_ms", 0)
        if isinstance(delay_ms, (int, float)) and delay_ms > 0:
            await asyncio.sleep(delay_ms / 1000.0)
            logger.debug(f"Applied simulation delay: {delay_ms}ms")
            return
    # Fallback to any hardcoded default delay if present (though we're removing them)
    elif default_delay_s > 0:
        await asyncio.sleep(default_delay_s)

# --- 2. Example Node Functions/Runnables ---
async def entry_point_node(state: BasicAgentState, config: Optional[Dict[str, Any]] = None) -> BasicAgentState:
    logger.info(f"Executing entry_point_node with config: {config}")
    await apply_simulation_delay(config) 
    if not state.get("messages"): 
        state["messages"] = []
    state["messages"].append(HumanMessage(content="Workflow started by entry_point_node"))
    state["next_node"] = "agent_node" 
    return state

async def simple_message_modifier_node(state: BasicAgentState, config: Optional[Dict[str, Any]] = None) -> BasicAgentState:
    node_id_for_log = config.get("node_id_for_log", "simple_message_modifier_node") if config else "simple_message_modifier_node"
    logger.info(f"Executing {node_id_for_log} with config: {config}")
    await apply_simulation_delay(config)
    prefix = config.get("message_prefix", "Modified: ") if config else "Modified: "
    if state.get("messages") and state["messages"]: # Check if messages list exists and is not empty
        last_message_content = state["messages"][-1].content
        state["messages"].append(AIMessage(content=f"{prefix}{last_message_content}"))
    else:
        if not state.get("messages"): state["messages"] = [] # Ensure messages list exists
        state["messages"].append(AIMessage(content=f"{prefix}No previous message."))
    return state

async def simulated_llm_node(state: DocumentProcessingState, config: Optional[Dict[str, Any]] = None) -> DocumentProcessingState:
    node_id_for_log = config.get("node_id_for_log", "simulated_llm_node") if config else "simulated_llm_node"
    logger.info(f"Executing {node_id_for_log} with config: {config}")
    await apply_simulation_delay(config) 
    action = config.get("action", "summarize") if config else "summarize"
    doc_to_process = state.processed_document or state.original_document
    # No default hardcoded delay, use simulation_delay_ms instead
    if action == "summarize":
        state.summary = f"Simulated summary of: {doc_to_process[:50]}..."
        state.confidence_score = 0.85
    elif action == "extract_keywords":
        state.keywords = [f"sim_kw_{i+1}" for i in range(config.get("num_keywords", 3))] # type: ignore
        state.confidence_score = 0.90
    else:
        state.error_info = f"Unknown LLM action: {action}"
        state.is_processed_successfully = False
        return state
    state.is_processed_successfully = True
    return state

async def simulated_tool_node(state: BasicAgentState, config: Optional[Dict[str, Any]] = None) -> BasicAgentState:
    tool_name = config.get("tool_name", "generic_tool") if config else "generic_tool"
    node_id_for_log = config.get("node_id_for_log", f"simulated_tool_node:{tool_name}") if config else f"simulated_tool_node:{tool_name}"
    logger.info(f"Executing {node_id_for_log} with config: {config}")
    await apply_simulation_delay(config) 
    tool_input_key = config.get("input_key", "tool_input") if config else "tool_input" 
    tool_output_key = config.get("output_key", "tool_output") if config else "tool_output"
    tool_input_value = "No specific input provided"
    if state.get("messages") and state["messages"] and tool_input_key == "last_message_content": # Check messages existence
        tool_input_value = state["messages"][-1].content
    elif state.get(tool_input_key): # type: ignore
        tool_input_value = state[tool_input_key] # type: ignore
    logger.info(f"Executing simulated_tool_node: {tool_name} with input: '{tool_input_value}'")
    # No default hardcoded delay, use simulation_delay_ms instead
    tool_result = f"Result from {tool_name} for input '{tool_input_value}'"
    if not state.get("messages"): state["messages"] = [] # Ensure messages list exists
    state["messages"].append(ToolMessage(content=tool_result, tool_call_id=tool_name)) # type: ignore
    state[tool_output_key] = tool_result # type: ignore
    state["last_tool_call"] = {"name": tool_name, "input": tool_input_value, "output": tool_result}
    return state

def route_based_on_llm_output(state: DocumentProcessingState, config: Optional[Dict[str, Any]] = None) -> str:
    logger.info(f"Executing router: route_based_on_llm_output. Confidence: {state.confidence_score}")
    decision_threshold = config.get("decision_threshold", 0.8) if config else 0.8
    if state.error_info: return "error_handler_path" 
    if state.confidence_score is not None and state.confidence_score >= decision_threshold: return "high_confidence_path"
    else: return "low_confidence_path"

def agent_router(state: BasicAgentState) -> str:
    if state.get("next_node"):
        next_val = state.pop("next_node") 
        logger.info(f"Agent router: Explicitly routing to '{next_val}'")
        return str(next_val)
    if state.get("messages") and state["messages"]: # Check messages existence
        last_message = state["messages"][-1]
        if isinstance(last_message, AIMessage) and not state.get("last_tool_call"):
            logger.info("Agent router: AIMessage, no tool call, routing to end.")
            return END
        elif isinstance(last_message, HumanMessage) or isinstance(last_message, ToolMessage):
            logger.info("Agent router: Human or Tool message, routing to agent_node.")
            return "agent_node"
    logger.info("Agent router: Defaulting to end.")
    return END 

# --- 3. Example Static Graph Construction ---
def create_example_document_workflow() -> CompiledGraph:
    workflow = StateGraph(DocumentProcessingState)

    async def _doc_processor_node(state: DocumentProcessingState, invoke_config: Optional[Dict[str, Any]]=None):
        node_specific_config = {"action": "summarize", "router_function_name": "document_confidence_router", "node_id_for_log": "doc_processor"}
        final_config = {**node_specific_config, **(invoke_config or {})}
        return await simulated_llm_node(state, config=final_config)

    async def _final_summary_node(state: DocumentProcessingState, invoke_config: Optional[Dict[str, Any]]=None):
        node_specific_config = {"action": "extract_keywords", "num_keywords": 5, "node_id_for_log": "final_summary_node"}
        final_config = {**node_specific_config, **(invoke_config or {})}
        return await simulated_llm_node(state, config=final_config)

    async def _error_handling_node_doc_workflow(state: DocumentProcessingState, invoke_config: Optional[Dict[str, Any]]=None):
        node_id_for_log = "error_handling_node"
        error_prefix = "StaticError: "
        logger.info(f"Executing {node_id_for_log} with effective config including invoke_config: {invoke_config}")
        await apply_simulation_delay(invoke_config)
        state.error_info = (state.error_info or "Error encountered during processing") + \
                           f" - {error_prefix}Document processing failed or led to error path."
        state.is_processed_successfully = False
        logger.warning(f"{node_id_for_log}: Document processing marked as failed. Error info: {state.error_info}")
        return state

    workflow.add_node("doc_processor", _doc_processor_node)
    workflow.add_node("final_summary_node", _final_summary_node)
    workflow.add_node("error_handling_node", _error_handling_node_doc_workflow)

    workflow.set_entry_point("doc_processor")

    # Define the router function explicitly for clarity and to potentially resolve type issues
    def _doc_processor_router(state: DocumentProcessingState) -> str:
        # Configuration for this specific routing instance
        router_config = {"decision_threshold": 0.7}
        return route_based_on_llm_output(state, config=router_config)

    workflow.add_conditional_edges(
        "doc_processor",
        _doc_processor_router,  # Use the explicitly defined router function
        {"high_confidence_path": "final_summary_node", "low_confidence_path": "error_handling_node", "error_handler_path": "error_handling_node"}
    )
    workflow.add_edge("error_handling_node", END)
    workflow.add_edge("final_summary_node", END)
    compiled_graph = workflow.compile()
    setattr(compiled_graph, 'compiled_at', datetime.utcnow())
    logger.info("Example document workflow compiled successfully.")
    return compiled_graph

def create_basic_agent_workflow() -> CompiledGraph:
    graph = StateGraph(BasicAgentState)

    async def _entry_node(state: BasicAgentState, invoke_config: Optional[Dict[str, Any]]=None):
        node_specific_config = {"node_id_for_log": "entry_node"} # Original lambda config was {}
        final_config = {**node_specific_config, **(invoke_config or {})}
        return await entry_point_node(state, config=final_config)

    async def _agent_node(state: BasicAgentState, invoke_config: Optional[Dict[str, Any]]=None):
        node_specific_config = {"message_prefix": "AgentMod: ", "node_id_for_log": "agent_node"}
        final_config = {**node_specific_config, **(invoke_config or {})}
        return await simple_message_modifier_node(state, config=final_config)

    async def _tool_node(state: BasicAgentState, invoke_config: Optional[Dict[str, Any]]=None):
        node_specific_config = {"tool_name": "static_tool", "node_id_for_log": "tool_node"}
        final_config = {**node_specific_config, **(invoke_config or {})}
        return await simulated_tool_node(state, config=final_config)

    graph.add_node("entry_node", _entry_node)
    graph.add_node("agent_node", _agent_node)
    graph.add_node("tool_node", _tool_node)

    graph.set_entry_point("entry_node")
    graph.add_conditional_edges("entry_node", agent_router, {"agent_node": "agent_node", "tool_node": "tool_node", END: END})
    graph.add_conditional_edges("agent_node", agent_router, {"tool_node": "tool_node", END: END})
    graph.add_edge("tool_node", "agent_node")
    compiled_graph = graph.compile()
    setattr(compiled_graph, 'compiled_at', datetime.utcnow())
    logger.info("Basic agent workflow compiled successfully.")
    return compiled_graph

# --- Registries ---
NODE_IMPLEMENTATIONS: Dict[str, callable] = { # type: ignore[type-arg]
    "entry_point": entry_point_node, "simple_modifier": simple_message_modifier_node,
    "llm_node": simulated_llm_node, "tool_node": simulated_tool_node,
}
ROUTER_IMPLEMENTATIONS: Dict[str, callable] = { # type: ignore[type-arg]
    "document_confidence_router": route_based_on_llm_output, "basic_agent_router": agent_router,
}
STATE_SCHEMAS: Dict[str, type[TypedDict] | type[BaseModel]] = { # type: ignore[type-arg]
    "BasicAgentState": BasicAgentState, "DocumentProcessingState": DocumentProcessingState,
}

# --- Enhanced STATIC_GRAPHS_METADATA ---
STATIC_GRAPHS_METADATA: Dict[str, Dict[str, Any]] = {
    "example_document_workflow": {
        "compiled_graph": create_example_document_workflow(),
        "state_schema_name": "DocumentProcessingState",
        "description": "A statically defined workflow for processing documents and generating summaries or keywords based on confidence.",
        "entry_point_node_id": "doc_processor",
        "nodes": [
            SchemaNodeDefinition(id="doc_processor", type="llm_node", config={"action": "summarize", "router_function_name": "document_confidence_router"}, ui_position=NodeUIPosition(x=100, y=200)),
            SchemaNodeDefinition(id="final_summary_node", type="llm_node", config={"action": "extract_keywords", "num_keywords": 5}, ui_position=NodeUIPosition(x=400, y=100)),
            SchemaNodeDefinition(id="error_handling_node", type="simple_modifier", config={"message_prefix": "StaticError: "}, ui_position=NodeUIPosition(x=400, y=300)),
        ],
        "edges": [], # No standard edges in this example, only conditional
        "conditional_edges": [
            SchemaConditionalEdgesDefinition(
                source_node_id="doc_processor",
                mappings=[
                    SchemaConditionalEdgeMapping(condition_name="high_confidence_path", target_node_id="final_summary_node"),
                    SchemaConditionalEdgeMapping(condition_name="low_confidence_path", target_node_id="error_handling_node"),
                    SchemaConditionalEdgeMapping(condition_name="error_handler_path", target_node_id="error_handling_node"),
                ]
            )
        ],
        "terminal_node_ids": ["final_summary_node", "error_handling_node"],
    },
    "basic_agent_workflow": {
        "compiled_graph": create_basic_agent_workflow(),
        "state_schema_name": "BasicAgentState",
        "description": "A basic agent that can use a simulated tool and modify messages.",
        "entry_point_node_id": "entry_node",
        "nodes": [
            SchemaNodeDefinition(id="entry_node", type="entry_point", config={}, ui_position=NodeUIPosition(x=100, y=100)),
            SchemaNodeDefinition(id="agent_node", type="simple_modifier", config={"message_prefix": "AgentMod: "}, ui_position=NodeUIPosition(x=300, y=100)),
            SchemaNodeDefinition(id="tool_node", type="tool_node", config={"tool_name": "static_tool"}, ui_position=NodeUIPosition(x=300, y=250)),
        ],
        "edges": [ # This graph has standard edges
            SchemaEdgeDefinition(id="e_tool_to_agent", source="tool_node", target="agent_node", label="Tool Output")
        ],
        "conditional_edges": [
            SchemaConditionalEdgesDefinition(
                source_node_id="entry_node",
                mappings=[
                    SchemaConditionalEdgeMapping(condition_name="agent_node", target_node_id="agent_node"),
                    SchemaConditionalEdgeMapping(condition_name="tool_node", target_node_id="tool_node"),
                    SchemaConditionalEdgeMapping(condition_name=END, target_node_id=END), # Target can be END
                ]
            ),
            SchemaConditionalEdgesDefinition(
                source_node_id="agent_node",
                mappings=[
                    SchemaConditionalEdgeMapping(condition_name="tool_node", target_node_id="tool_node"),
                    SchemaConditionalEdgeMapping(condition_name=END, target_node_id=END),
                ]
            ),
        ],
        "terminal_node_ids": [], # END is handled by conditional edges directly
    },
}

STATIC_GRAPHS: Dict[str, CompiledGraph] = {
    name: data["compiled_graph"] for name, data in STATIC_GRAPHS_METADATA.items()
}

File: rag/langgraph_vis/stream_handler.py
# rag/langgraph_vis/stream_handler.py
import asyncio
import json
import logging
import uuid
from typing import AsyncGenerator, Dict, Any, Optional
from datetime import datetime

from fastapi import APIRouter, HTTPException, Body
from fastapi.responses import StreamingResponse

from .schemas import (
    GraphExecutionStartEvent,
    NodeStartEvent,
    NodeEndEvent,
    EdgeTakenEvent,
    GraphExecutionEndEvent,
    GraphErrorEvent,
    ExecuteGraphRequest,
)
from .core.builder import DynamicGraphBuilder, DynamicGraphBuilderError
from .core.definitions import STATIC_GRAPHS_METADATA
from .api_routes import _load_graph_definition_from_file

logger = logging.getLogger(__name__)
router = APIRouter()

async def stream_graph_execution(
    graph_id: str,
    execution_id: str,
    input_args: Dict[str, Any],
    simulation_delay_ms: Optional[int] = None
) -> AsyncGenerator[bytes, None]:
    """Stream NDJSON (newline-delimited JSON) events for graph execution."""
    
    def serialize_event(event: Any) -> bytes:
        """Serialize event to NDJSON format (JSON + newline)."""
        return (event.model_dump_json(by_alias=True) + '\n').encode('utf-8')
    
    try:
        # Load and build the graph
        compiled_graph = None
        if graph_id.startswith("static_"):
            static_graph_name = graph_id[len("static_"):]
            if static_graph_name in STATIC_GRAPHS_METADATA:
                compiled_graph = STATIC_GRAPHS_METADATA[static_graph_name]["compiled_graph"]
            else:
                error_event = GraphErrorEvent(
                    execution_id=execution_id,
                    graph_id=graph_id,
                    message=f"Static graph '{static_graph_name}' not found.",
                    timestamp=datetime.utcnow()
                )
                yield serialize_event(error_event)
                return
        else:
            graph_def = _load_graph_definition_from_file(graph_id)
            if not graph_def:
                error_event = GraphErrorEvent(
                    execution_id=execution_id,
                    graph_id=graph_id,
                    message=f"Graph definition ID '{graph_id}' not found.",
                    timestamp=datetime.utcnow()
                )
                yield serialize_event(error_event)
                return
            
            try:
                compiled_graph = DynamicGraphBuilder(graph_def).build()
            except DynamicGraphBuilderError as e:
                error_event = GraphErrorEvent(
                    execution_id=execution_id,
                    graph_id=graph_id,
                    message=f"Failed to build graph: {str(e)}",
                    timestamp=datetime.utcnow()
                )
                yield serialize_event(error_event)
                return

        # Send start event
        start_event = GraphExecutionStartEvent(
            execution_id=execution_id,
            graph_id=graph_id,
            input_args=input_args
        )
        yield serialize_event(start_event)

        # Prepare execution config
        execution_config = {"recursion_limit": 25}
        if simulation_delay_ms is not None:
            execution_config["simulation_delay_ms"] = simulation_delay_ms

        # Stream graph execution events
        async for event_chunk in compiled_graph.astream_events(
            input_args,
            version="v2",
            config=execution_config
        ):
            event_type = event_chunk["event"]
            event_data = event_chunk.get("data", {})
            event_name = event_chunk.get("name", "")
            tags = event_chunk.get("tags", [])

            # Map LangGraph events to our streaming events
            if "langgraph:node:" + event_name in tags:
                if event_type in ["on_chain_start", "on_tool_start", "on_chat_model_start"]:
                    node_start_event = NodeStartEvent(
                        execution_id=execution_id,
                        graph_id=graph_id,
                        node_id=event_name,
                        input_data=event_data.get("input", event_data.get("input_str", {}))
                    )
                    yield serialize_event(node_start_event)
                
                elif event_type in ["on_chain_end", "on_tool_end", "on_chat_model_end"]:
                    status = "success"
                    error_msg = None
                    output = event_data.get("output", {})
                    
                    if isinstance(output, Exception):
                        status = "failure"
                        error_msg = str(output)
                    
                    node_end_event = NodeEndEvent(
                        execution_id=execution_id,
                        graph_id=graph_id,
                        node_id=event_name,
                        output_data=output if status == "success" else {"error": error_msg},
                        status=status,
                        error_message=error_msg
                    )
                    yield serialize_event(node_end_event)

            # Optional: Send edge events if you can detect them
            if "langgraph:edge" in tags:
                # Parse edge information from event
                edge_event = EdgeTakenEvent(
                    execution_id=execution_id,
                    graph_id=graph_id,
                    source_node_id=event_data.get("source", "unknown"),
                    target_node_id=event_data.get("target", "unknown"),
                    edge_label=event_data.get("label"),
                    is_conditional=event_data.get("conditional", False)
                )
                yield serialize_event(edge_event)

        # Send completion event
        final_state_data = event_chunk.get("data", {}).get("output", {}) if 'event_chunk' in locals() else {}
        graph_end_event = GraphExecutionEndEvent(
            execution_id=execution_id,
            graph_id=graph_id,
            final_state=final_state_data,
            status="completed"
        )
        yield serialize_event(graph_end_event)

    except Exception as e:
        logger.error(f"Error during stream for execution '{execution_id}': {e}", exc_info=True)
        error_event = GraphErrorEvent(
            execution_id=execution_id,
            graph_id=graph_id,
            message="An unexpected error occurred during graph execution.",
            details=str(e),
            timestamp=datetime.utcnow()
        )
        yield serialize_event(error_event)

@router.post("/graphs/{graph_id}/execute/stream")
async def execute_graph_stream(
    graph_id: str,
    request: ExecuteGraphRequest = Body(...)
):
    """Execute a graph and stream events as NDJSON."""
    execution_id = f"stream_exec_{uuid.uuid4().hex[:12]}"
    
    logger.info(f"Stream execution request for graph '{graph_id}' with execution_id '{execution_id}'")
    
    return StreamingResponse(
        stream_graph_execution(
            graph_id,
            execution_id,
            request.input_args or {},
            request.simulation_delay_ms
        ),
        media_type="application/x-ndjson",
        headers={
            "Cache-Control": "no-cache",
            "X-Execution-ID": execution_id,
            "X-Content-Type-Options": "nosniff",  # Security header
        }
    )

File: rag/db/postgres_reader.py
# rag/db/postgres_reader.py
import logging
from typing import List, Dict, Any
from sqlalchemy import create_engine, text

from rag.config import config # Import the global config

logger = logging.getLogger("postgres_reader")

DATABASE_URL = f"postgresql+psycopg2://{config.postgres.user}:{config.postgres.password}@{config.postgres.host}:{config.postgres.port}/{config.postgres.dbname}"

try:
    engine = create_engine(DATABASE_URL, pool_pre_ping=True)
    logger.info(f"SQLAlchemy engine created for {config.postgres.host}:{config.postgres.port}/{config.postgres.dbname}")
except Exception as e:
    logger.error(f"Failed to create SQLAlchemy engine: {e}", exc_info=True)
    engine = None # Set engine to None if creation fails

def fetch_unspsc_commodities() -> List[Dict[str, Any]]:
    """Fetches UNSPSC Commodity level categories from the PostgreSQL database."""
    if engine is None:
        logger.error("Database engine not initialized. Cannot fetch UNSPSC commodities.")
        return []

    # Find the system ID for UNSPSC
    system_id_query = text("SELECT id FROM classification_systems WHERE code = 'UNSPSC' LIMIT 1")
    # Find the level code for Commodity (assuming it's level 4 and code 'commodity')
    commodity_level_code = 'commodity' # Assuming this is the code stored in classification_levels

    items = []
    try:
        with engine.connect() as connection:
            logger.info("Fetching UNSPSC System ID...")
            result = connection.execute(system_id_query)
            system_id_row = result.fetchone()
            if not system_id_row:
                logger.error("UNSPSC system not found in classification_systems table.")
                return []
            unspsc_system_id = system_id_row[0]
            logger.info(f"Found UNSPSC System ID: {unspsc_system_id}")

            # Fetch commodities for the UNSPSC system
            commodity_query = text(f"""
                SELECT code, name, description
                FROM categories
                WHERE system_id = :system_id AND level_code = :level_code
            """)
            logger.info(f"Fetching categories for system_id={unspsc_system_id}, level_code='{commodity_level_code}'...")
            result = connection.execute(commodity_query, {"system_id": unspsc_system_id, "level_code": commodity_level_code})

            for row in result:
                items.append({
                    "code": row[0],
                    "name": row[1],
                    "description": row[2] if row[2] else row[1] # Use name if description is null
                })
            logger.info(f"Fetched {len(items)} UNSPSC commodity categories.")

    except Exception as e:
        logger.error(f"Error fetching UNSPSC commodities from PostgreSQL: {e}", exc_info=True)
        return [] # Return empty list on error

    return items

File: rag/langgraph_vis/core/builder.py
# rag/langgraph_vis/core/builder.py
import logging
from typing import Dict, Any, Type, Callable # Import Callable
import asyncio # Import asyncio
import functools # Import functools

from langgraph.graph import StateGraph, END
from langgraph.graph.graph import CompiledGraph
from pydantic import BaseModel

from ..schemas import GraphDefinition, NodeDefinition, EdgeDefinition, ConditionalEdgesDefinition
from .definitions import STATE_SCHEMAS, NODE_IMPLEMENTATIONS, ROUTER_IMPLEMENTATIONS

logger = logging.getLogger(__name__)

class DynamicGraphBuilderError(Exception):
    pass

class DynamicGraphBuilder:
    def __init__(self, graph_definition_data: Dict[str, Any] | GraphDefinition):
        if isinstance(graph_definition_data, GraphDefinition):
            self.graph_definition = graph_definition_data
        elif isinstance(graph_definition_data, dict):
            try:
                self.graph_definition = GraphDefinition(**graph_definition_data)
            except Exception as e:
                logger.error(f"Failed to parse graph definition dictionary: {e}")
                raise DynamicGraphBuilderError(f"Invalid graph definition data: {e}")
        else:
            raise DynamicGraphBuilderError("graph_definition_data must be a dict or GraphDefinition model.")
        self._validate_definition()
        logger.info(f"DynamicGraphBuilder initialized for graph: '{self.graph_definition.name}' (ID: {self.graph_definition.id})")

    def _validate_definition(self) -> None:
        if not self.graph_definition.nodes:
            raise DynamicGraphBuilderError("Graph definition must contain at least one node.")
        if not self.graph_definition.entry_point_node_id:
            raise DynamicGraphBuilderError("Graph definition must specify an entry_point_node_id.")
        node_ids = {node.id for node in self.graph_definition.nodes}
        if self.graph_definition.entry_point_node_id not in node_ids:
            raise DynamicGraphBuilderError(
                f"Entry point node ID '{self.graph_definition.entry_point_node_id}' not found in defined nodes."
            )
        if self.graph_definition.state_schema_name not in STATE_SCHEMAS:
            raise DynamicGraphBuilderError(
                f"Unknown state_schema_name: '{self.graph_definition.state_schema_name}'. "
                f"Available schemas: {list(STATE_SCHEMAS.keys())}"
            )
        logger.debug("Graph definition basic validation passed.")

    def build(self) -> CompiledGraph:
        logger.info(f"Starting to build graph: '{self.graph_definition.name}'")
        StateClass: Type[BaseModel | dict] = STATE_SCHEMAS[self.graph_definition.state_schema_name] # type: ignore
        workflow = StateGraph(StateClass) # type: ignore
        node_ids_with_outgoing_edges = set()
        for node_def in self.graph_definition.nodes:
            self._add_node_to_workflow(workflow, node_def)
        for edge_def in self.graph_definition.edges:
            workflow.add_edge(edge_def.source, edge_def.target)
            node_ids_with_outgoing_edges.add(edge_def.source)
            logger.debug(f"Added standard edge from '{edge_def.source}' to '{edge_def.target}'.")
        for cond_edges_def in self.graph_definition.conditional_edges:
            self._add_conditional_edges_to_workflow(workflow, cond_edges_def)
            node_ids_with_outgoing_edges.add(cond_edges_def.source_node_id)
        workflow.set_entry_point(self.graph_definition.entry_point_node_id)
        logger.debug(f"Set entry point to '{self.graph_definition.entry_point_node_id}'.")
        all_node_ids = {node.id for node in self.graph_definition.nodes}
        defined_terminal_node_ids = set(self.graph_definition.terminal_node_ids or [])
        for node_id in all_node_ids:
            is_explicitly_terminal = node_id in defined_terminal_node_ids
            has_no_outgoing = node_id not in node_ids_with_outgoing_edges
            if is_explicitly_terminal or has_no_outgoing:
                if node_id != self.graph_definition.entry_point_node_id or \
                   (node_id == self.graph_definition.entry_point_node_id and node_id in node_ids_with_outgoing_edges):
                    if has_no_outgoing and node_id not in defined_terminal_node_ids:
                         logger.debug(f"Node '{node_id}' has no outgoing edges, adding implicit edge to END.")
                         workflow.add_edge(node_id, END)
                    elif is_explicitly_terminal:
                         logger.debug(f"Node '{node_id}' is explicitly terminal, adding edge to END.")
                         workflow.add_edge(node_id, END)
        try:
            compiled_graph = workflow.compile()
            logger.info(f"Graph '{self.graph_definition.name}' built and compiled successfully.")
            return compiled_graph
        except Exception as e:
            logger.error(f"Failed to compile graph '{self.graph_definition.name}': {e}", exc_info=True)
            raise DynamicGraphBuilderError(f"Error compiling graph: {e}")

    def _add_node_to_workflow(self, workflow: StateGraph, node_def: NodeDefinition) -> None:
        node_id = node_def.id
        node_type = node_def.type
        # node_config is the config from the GraphDefinition JSON for this specific node
        node_specific_config = node_def.config if node_def.config else {}

        if node_type not in NODE_IMPLEMENTATIONS:
            raise DynamicGraphBuilderError(
                f"Unknown node type: '{node_type}' for node ID '{node_id}'. "
                f"Available types: {list(NODE_IMPLEMENTATIONS.keys())}"
            )

        original_node_callable: Callable = NODE_IMPLEMENTATIONS[node_type] # type: ignore
        final_node_callable: Callable # type: ignore

        # This is where the global simulation_delay_ms (if any) needs to be merged
        # with the node's specific config.
        # The `execution_config` in `ws_handler.py` is the top-level config for the graph run.
        # LangGraph's `astream_events` `config` parameter is passed to each node's underlying runnable.

        if asyncio.iscoroutinefunction(original_node_callable):
            async def async_wrapper(state: Any, runnable_config: Dict[str, Any] = None) -> Any: # LangGraph passes runnable_config
                # Merge node_specific_config and runnable_config (runtime config)
                # Runtime config (like simulation_delay_ms) should take precedence if there are clashes.
                merged_config = {**node_specific_config, **(runnable_config or {})}
                # The node functions need to be aware of this.
                # For example, `simulation_delay_ms` comes from runnable_config.
                # `message_prefix` comes from node_specific_config.
                logger.debug(f"AsyncWrapper for '{node_id}': node_specific_config={node_specific_config}, runtime_runnable_config={runnable_config}, merged_config={merged_config}")
                return await original_node_callable(state, config=merged_config)
            final_node_callable = async_wrapper
        else:
            def sync_wrapper(state: Any, runnable_config: Dict[str, Any] = None) -> Any:
                merged_config = {**node_specific_config, **(runnable_config or {})}
                logger.debug(f"SyncWrapper for '{node_id}': node_specific_config={node_specific_config}, runtime_runnable_config={runnable_config}, merged_config={merged_config}")
                return original_node_callable(state, config=merged_config)
            final_node_callable = sync_wrapper
        
        workflow.add_node(node_id, final_node_callable) # type: ignore
        logger.debug(f"Added node '{node_id}' of type '{node_type}' with its specific definition config: {node_specific_config}.")

    def _add_conditional_edges_to_workflow(
        self, workflow: StateGraph, cond_edges_def: ConditionalEdgesDefinition
    ) -> None:
        source_node_id = cond_edges_def.source_node_id
        mappings = {mapping.condition_name: mapping.target_node_id for mapping in cond_edges_def.mappings}
        source_node_def = next((n for n in self.graph_definition.nodes if n.id == source_node_id), None)
        if not source_node_def:
            raise DynamicGraphBuilderError(f"Source node '{source_node_id}' for conditional edges not found.")
        router_config = source_node_def.config or {}
        router_function_name = router_config.get("router_function_name")
        if not router_function_name:
            raise DynamicGraphBuilderError(
                f"Node '{source_node_id}' is a source for conditional edges but does not specify "
                f"a 'router_function_name' in its config."
            )
        if router_function_name not in ROUTER_IMPLEMENTATIONS:
            raise DynamicGraphBuilderError(
                f"Unknown router_function_name: '{router_function_name}' for node '{source_node_id}'. "
                f"Available routers: {list(ROUTER_IMPLEMENTATIONS.keys())}"
            )
        
        original_router_callable: Callable = ROUTER_IMPLEMENTATIONS[router_function_name] # type: ignore
        final_router_callable: Callable # type: ignore

        # Routers are typically synchronous as they inspect state, but handle async just in case
        if asyncio.iscoroutinefunction(original_router_callable):
            async def async_router_wrapper(state: Any, runnable_config: Dict[str, Any] = None) -> Any:
                # Merge router_config and runnable_config (runtime config)
                merged_config = {**router_config, **(runnable_config or {})}
                logger.debug(f"AsyncRouterWrapper for '{source_node_id}': router_config={router_config}, runtime_runnable_config={runnable_config}, merged_config={merged_config}")
                return await original_router_callable(state, config=merged_config)
            final_router_callable = async_router_wrapper
        else:
            def sync_router_wrapper(state: Any, runnable_config: Dict[str, Any] = None) -> Any:
                merged_config = {**router_config, **(runnable_config or {})}
                logger.debug(f"SyncRouterWrapper for '{source_node_id}': router_config={router_config}, runtime_runnable_config={runnable_config}, merged_config={merged_config}")
                return original_router_callable(state, config=merged_config)
            final_router_callable = sync_router_wrapper
            
        workflow.add_conditional_edges(source_node_id, final_router_callable, mappings) # type: ignore
        logger.debug(f"Added conditional edges from '{source_node_id}' using router '{router_function_name}' with mappings: {mappings}.")


File: rag/db/__init__.py
# Database package 


File: rag/api/models.py
# rag/api/models.py
"""Pydantic models for API requests and responses."""
from typing import Dict, List, Optional
from pydantic import BaseModel, Field, ConfigDict
from datetime import datetime # Import datetime

# --- Keep existing models (Token, NewUserRequest, CategoryItem, etc.) ---
class Token(BaseModel):
    access_token: str
    token_type: str

class NewUserRequest(BaseModel):
    username: str
    password: str
    disabled: bool = False

# Item models
class CategoryItem(BaseModel):
    code: str
    name: str
    description: str
    hierarchy: str = ""
    metadata: Dict = Field(default_factory=dict)

class BatchAddRequest(BaseModel):
    items: List[CategoryItem]
    collection_name: str

# Search models
class SimilarityResult(BaseModel):
    code: str
    name: str
    description: str = ""  # Add description field with default empty string
    hierarchy: str
    similarity_score: float
    metadata: Dict

class SimilarityResponse(BaseModel):
    query: str
    collection_name: str
    results: List[SimilarityResult]

class MultiCollectionSearchResponse(BaseModel):
    query: str
    results: Dict[str, List[SimilarityResult]]

# Collection models
class CollectionInfo(BaseModel):
    name: str
    count: int

class ListCollectionsResponse(BaseModel):
    collections: List[CollectionInfo]

# Status models
class StatusResponse(BaseModel):
    status: str
    chroma_connected: bool
    collections: List[str]
    auth_enabled: bool

class User(BaseModel):
    username: str
    disabled: Optional[bool] = None

class UserInDB(User):
    hashed_password: str

# --- RAG Info Models ---
class RagInfoItemBase(BaseModel):
    key: str = Field(..., description="Unique key for the information")
    description: str = Field(..., description="The textual information content")

class RagInfoItemCreate(RagInfoItemBase):
    pass # Same fields as base for creation

class RagInfoItemUpdate(BaseModel):
    # Only description is updatable via the frontend modal
    description: str = Field(..., description="The updated textual information")

class RagInfoItem(RagInfoItemBase):
    id: str = Field(..., description="Unique identifier (same as key in this implementation)")
    createdAt: datetime = Field(..., description="Creation timestamp")
    updatedAt: datetime = Field(..., description="Last update timestamp")

    # Use model_config for Pydantic V2
    model_config = ConfigDict(from_attributes=True)

class RagInfoPageResponse(BaseModel):
    items: List[RagInfoItem]
    totalCount: int = Field(..., description="Total number of items matching filters")
    totalPages: int = Field(..., description="Total number of pages")
    currentPage: int = Field(..., description="The current page number (1-based)")

# --- Chat Models ---
class ChatMessagePy(BaseModel):
    id: Optional[str] = None
    role: str # "user", "assistant", "system"
    content: str
    # createdAt: Optional[datetime] = None # Keep it simple for now

class GenAIChatRequest(BaseModel):
    messages: List[ChatMessagePy]
    stream: Optional[bool] = Field(default=True)
    model: Optional[str] = None
    # max_tokens: Optional[int] = None # Example: if you want to pass max_tokens

class GenAIChatResponseChunk(BaseModel):
    text: Optional[str] = None
    done: bool
    error: Optional[str] = None

File: rag/api/__init__.py
# API package 


File: rag/db/vector_store.py
"""ChromaDB vector store interaction."""
from http.client import HTTPException
import logging
import os
import requests
from typing import List, Dict, Any, Optional, Tuple
import time
from datetime import datetime, timezone

import chromadb
from chromadb.utils import embedding_functions
from chromadb.config import Settings
from requests.exceptions import ConnectionError

from rag.config import config

# Use configured values
CHROMA_HOST = config.chromadb.host
CHROMA_PORT = config.chromadb.port
DEFAULT_COLLECTION = config.chromadb.default_collection
MANUAL_INFO_COLLECTION = config.chromadb.manual_info_collection

logger = logging.getLogger("vector_store")

# --- Metadata Keys ---
# Use constants for metadata field names for consistency and easier refactoring
META_TYPE = "item_type"       # To distinguish manual info from categories etc.
META_KEY = "original_key"    # Store the original key in metadata for filtering
META_CREATED_AT = "created_at_iso"
META_UPDATED_AT = "updated_at_iso"
TYPE_MANUAL = "manual_info"

class VectorStore:
    """ChromaDB vector store wrapper."""
    
    def __init__(self, host: Optional[str] = None, port: Optional[int] = None):
        """Initialize ChromaDB client."""
        self.host = host or CHROMA_HOST
        self.port = port or CHROMA_PORT
        # Consider making embedding function configurable if needed
        self.embedding_function = embedding_functions.DefaultEmbeddingFunction()
        self.manual_info_collection_name = MANUAL_INFO_COLLECTION # Store configured name
        self.client = self._initialize_client()
        # Run migrations after client is fully initialized
        try:
            self._migrate_manual_info_metadata()
        except Exception as e:
            logger.error(f"Error during metadata migration: {e}")

    def _migrate_manual_info_metadata(self):
        """Migrates existing manual info items to ensure they have all required metadata fields."""
        collection = self._get_manual_info_collection()
        try:
            # Get all manual info items
            results = collection.get(
                where={META_TYPE: TYPE_MANUAL},
                include=['metadatas', 'documents']
            )
            
            if not results or not results.get('ids') or not results['ids']:
                logger.info("No manual info items found for metadata migration.")
                return
            
            updated_count = 0
            for i in range(len(results['ids'])):
                doc_id = results['ids'][i]
                document = results['documents'][i] if results.get('documents') and i < len(results['documents']) else ""
                metadata = results['metadatas'][i] if results.get('metadatas') and i < len(results['metadatas']) else {}
                
                # Check if name field is missing from metadata
                if 'name' not in metadata:
                    # Update metadata with name field
                    updated_metadata = metadata.copy()
                    updated_metadata['name'] = doc_id  # Use document ID (key) as name
                    
                    # Update the item
                    collection.update(
                        ids=[doc_id],
                        metadatas=[updated_metadata]
                    )
                    updated_count += 1
                    logger.info(f"Migrated metadata for item: {doc_id}, added name field")
            
            if updated_count > 0:
                logger.info(f"Migration completed: updated {updated_count} manual info items with name field")
            else:
                logger.info("No manual info items needed metadata migration.")
                
        except Exception as e:
            logger.error(f"Error during manual info metadata migration: {e}")

    def _initialize_client(self):
        """Initialize ChromaDB client."""
        try:
            self._check_server_availability()
            logger.info(f"Attempting to connect to ChromaDB HTTP Client at {self.host}:{self.port}")
            client = chromadb.HttpClient(
                host=self.host,
                port=self.port,
                # Explicitly specify tenant and database parameters
                tenant="default_tenant",
                database="default_database"
            )
            client.heartbeat() # Test connection
            logger.info("Successfully connected to ChromaDB server via HTTP.")
            
            return client
        except Exception as e:
            logger.warning(f"Failed to connect to ChromaDB server: {e}. Using persistent local client.")
            # Use a persistent directory instead of in-memory
            persist_dir = os.path.join("data", "chroma", "db")
            # Ensure directory exists
            os.makedirs(persist_dir, exist_ok=True)
            logger.info(f"Using persistent ChromaDB client with directory: {persist_dir}")
            # Create client with persistence
            client = chromadb.Client(Settings(persist_directory=persist_dir))
            
            return client

    def _check_server_availability(self, retries: int = 5, delay: int = 3, timeout: int = 5):
        """Check if ChromaDB server is available, with retries."""
        for attempt in range(retries):
            try:
                logger.info(f"Attempting to connect to ChromaDB server (attempt {attempt + 1}/{retries} at http://{self.host}:{self.port})...")
                response = requests.get(f"http://{self.host}:{self.port}/api/v1/heartbeat", timeout=timeout)
                response.raise_for_status()  # Raises error for bad status (4xx or 5xx)
                logger.info("ChromaDB server heartbeat check successful.")
                return True
            except requests.exceptions.RequestException as e: # Catch specific requests exceptions
                logger.warning(f"ChromaDB server heartbeat check attempt {attempt + 1}/{retries} failed: {e}")
                if attempt < retries - 1:
                    logger.info(f"Waiting {delay} seconds before next attempt...")
                    time.sleep(delay) # Ensure time module is imported (it is in the original file)
                else:
                    logger.error(f"All {retries} attempts to connect to ChromaDB server failed.")
                    # Raise ConnectionError to be caught by _initialize_client for fallback
                    raise ConnectionError(f"Cannot connect to ChromaDB at http://{self.host}:{self.port} after {retries} attempts") from e
        # This line should ideally not be reached if retries > 0, as ConnectionError would be raised.
        # However, to satisfy linters or strict type checking that expect a boolean return path:
        return False


    def test_connection(self) -> bool:
        """Test the connection to ChromaDB."""
        try:
            self.client.heartbeat()
            return True
        except Exception as e:
            logger.error(f"ChromaDB connection error: {e}")
            return False

    def _get_manual_info_collection(self):
        """Helper to get the specific collection for manual RAG info."""
        return self.client.get_or_create_collection(
            name=self.manual_info_collection_name,
            embedding_function=self.embedding_function
        )
    
    def list_collections(self) -> List[Dict[str, Any]]:
        """List all collections with their details."""
        collections = []
        try:
            for collection in self.client.list_collections():
                try:
                    col = self.client.get_collection(collection.name, embedding_function=self.embedding_function)
                    count = col.count()
                    collections.append({
                        "name": collection.name,
                        "count": count
                    })
                except Exception as e:
                    logger.warning(f"Error getting details for collection {collection.name}: {e}")
                    collections.append({
                        "name": collection.name,
                        "count": -1  # Error indicator
                    })
        except Exception as e:
            logger.error(f"Error listing collections: {e}")
        return collections
    
    def get_collection(self, collection_name: str, create_if_not_exists: bool = True):
        """Get or create a collection."""
        try:
            # Always try get_or_create if create_if_not_exists is True
            if create_if_not_exists:
                 logger.info(f"Getting or creating collection: {collection_name}")
                 return self.client.get_or_create_collection(
                     name=collection_name,
                     embedding_function=self.embedding_function
                 )
            else:
                 # Only get if not creating
                 logger.info(f"Getting existing collection: {collection_name}")
                 return self.client.get_collection(
                     name=collection_name,
                     embedding_function=self.embedding_function
                 )
        except Exception as e:
             logger.error(f"Error accessing collection {collection_name}: {e}")
             raise # Re-raise the exception
    
    def delete_collection(self, collection_name: str) -> bool:
        """Delete a collection.
        
        Args:
            collection_name: Name of the collection to delete
            
        Returns:
            True if successful, False otherwise
        """
        try:
            self.client.delete_collection(collection_name)
            return True
        except Exception as e:
            logger.error(f"Error deleting collection {collection_name}: {e}")
            return False
    
    def add_items(self, collection_name: str, items: List[Dict[str, Any]]) -> int:
        """Add items to a collection.
        
        Args:
            collection_name: Name of the collection
            items: List of items to add, each containing code, name, description, etc.
            
        Returns:
            Number of items added
        """
        collection = self.get_collection(collection_name)
        
        # Prepare data for ChromaDB
        ids = [item["code"] for item in items]
        documents = [item["description"] for item in items]
        metadatas = []
        
        for item in items:
            # Create metadata
            metadata = {
                "code": item["code"],
                "name": item["name"],
                "hierarchy": item.get("hierarchy", "")
            }
            # Add any additional metadata
            if "metadata" in item:
                metadata.update(item["metadata"])
            metadatas.append(metadata)
        
        # Add to ChromaDB
        collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
        
        return len(items)
    
    def search(
        self,
        collection_name: str,
        query: str,
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """Search for similar items in a collection."""
        logger.info(f"Vector store searching for '{query}' in collection '{collection_name}' with limit {limit}")
        
        collection = self.get_collection(collection_name, create_if_not_exists=False) # Don't create on search

        results = collection.query(
            query_texts=[query],
            n_results=limit,
            include=['metadatas', 'documents', 'distances'] # Ensure necessary fields are included
        )

        formatted_results = []
        if results and results.get("ids") and results["ids"][0]: # Check if results are valid
             ids = results["ids"][0]
             metadatas = results.get("metadatas", [[]])[0]
             documents = results.get("documents", [[]])[0]
             distances = results.get("distances", [[]])[0]
             
             result_count = len(ids)
             logger.info(f"Vector store found {result_count} results for '{query}' in '{collection_name}'")

             for i in range(len(ids)):
                 metadata = metadatas[i] if i < len(metadatas) else {}
                 document = documents[i] if i < len(documents) else ""
                 
                 # Calculate similarity score (1 - distance) for cosine distance
                 # For L2 distance, similarity might need a different formula or just use distance
                 similarity = 0.0
                 if i < len(distances) and distances[i] is not None:
                    # Assuming default embedding function uses cosine distance where lower is better
                    similarity = 1.0 - distances[i]

                 formatted_results.append({
                     "code": ids[i], # ID is the code for categories
                     "name": metadata.get("name", ""),
                     "description": document,  # Include the actual document content as description
                     "hierarchy": metadata.get("hierarchy", ""),
                     "similarity_score": similarity,
                     "metadata": { # Exclude common fields from general metadata
                         k: v for k, v in metadata.items()
                         if k not in ["name", "hierarchy", "code"]
                     }
                 })
                 
                 # Log top 3 results with more detail
                 if i < 3:
                     doc_excerpt = document[:50] + "..." if document and len(document) > 50 else document
                     logger.debug(f"Result {i+1}: code={ids[i]}, name='{metadata.get('name', '')}', similarity={similarity:.3f}, document='{doc_excerpt}'")
                     
        else:
            logger.info(f"No results found for '{query}' in '{collection_name}'")
            
        return formatted_results
    
    def search_all_collections(
        self, 
        query: str, 
        limit_per_collection: int = 3,
        min_score: float = 0.0
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Search for similar items across all collections."""
        logger.info(f"Vector store searching all collections for query: '{query}', limit_per_collection: {limit_per_collection}, min_score: {min_score}")
        
        all_results = {}
        collections = self.client.list_collections()
        logger.info(f"Searching across {len(collections)} collections")
        
        for collection in collections:
            # Skip the manual info collection in this generic search
            if collection.name == self.manual_info_collection_name:
                continue
            try:
                logger.debug(f"Searching collection '{collection.name}' for '{query}'")
                results = self.search(
                    collection_name=collection.name,
                    query=query,
                    limit=limit_per_collection
                )
                
                # Filter by minimum score
                filtered_results = [r for r in results if r["similarity_score"] >= min_score]
                
                if filtered_results:
                    all_results[collection.name] = filtered_results
                    logger.info(f"Found {len(filtered_results)} results in '{collection.name}' for '{query}' (after min_score filtering)")
                else:
                    logger.info(f"No results with similarity >= {min_score} found in '{collection.name}' for '{query}'")
            except Exception as e:
                logger.warning(f"Error searching collection '{collection.name}' for '{query}': {e}")
                continue
        
        total_results_count = sum(len(results) for results in all_results.values())
        logger.info(f"Completed search_all_collections for '{query}'. Total results: {total_results_count} across {len(all_results)} collections")
        return all_results

    # --- New Methods for Manual RAG Info ---

    def add_manual_info(self, key: str, description: str) -> Dict[str, Any]:
        """Adds a manual information item to its specific collection."""
        collection = self._get_manual_info_collection()
        now_iso = datetime.now(timezone.utc).isoformat()

        metadata = {
            META_TYPE: TYPE_MANUAL,
            META_KEY: key, # Store original key in metadata too
            META_CREATED_AT: now_iso,
            META_UPDATED_AT: now_iso,
            "name": key,  # Add name field to metadata with key as value
        }
        doc_id = key # Use the key as the document ID

        try:
            collection.add(
                ids=[doc_id],
                documents=[description],
                metadatas=[metadata]
            )
            logger.info(f"Added manual info item with key: {key}, description: '{description[:50]}{'...' if len(description) > 50 else ''}'")
            return {
                "id": doc_id,
                "key": key,
                "description": description,
                "createdAt": now_iso,
                "updatedAt": now_iso
            }
        except Exception as e: # Catch potential duplicate ID errors etc.
             logger.error(f"Error adding manual info for key '{key}': {e}")
             # Re-raise or handle specific exceptions (like DuplicateIdError if library provides it)
             raise HTTPException(status_code=409, detail=f"Item with key '{key}' might already exist.") from e


    def get_manual_info(self, key: str) -> Optional[Dict[str, Any]]:
        """Gets a manual information item by its key."""
        collection = self._get_manual_info_collection()
        try:
            result = collection.get(ids=[key], include=['metadatas', 'documents'])
            if not result or not result.get('ids') or not result['ids']:
                logger.warning(f"Manual info item not found for key: {key}")
                return None

            doc_id = result['ids'][0]
            document = result['documents'][0] if result.get('documents') else ""
            metadata = result['metadatas'][0] if result.get('metadatas') else {}

            # Verify it's the correct type
            if metadata.get(META_TYPE) != TYPE_MANUAL:
                 logger.warning(f"Retrieved item with key '{key}' but it's not of type '{TYPE_MANUAL}'")
                 return None


            return {
                "id": doc_id,
                "key": metadata.get(META_KEY, doc_id), # Prefer original key from metadata
                "description": document,
                "createdAt": metadata.get(META_CREATED_AT),
                "updatedAt": metadata.get(META_UPDATED_AT)
            }
        except Exception as e:
            logger.error(f"Error getting manual info for key '{key}': {e}")
            return None

    def update_manual_info(self, key: str, description: str) -> Optional[Dict[str, Any]]:
        """Updates the description of a manual information item."""
        collection = self._get_manual_info_collection()
        now_iso = datetime.now(timezone.utc).isoformat()

        # First, verify the item exists and get its current metadata
        existing_item = self.get_manual_info(key)
        if not existing_item:
            return None # Item not found

        # Prepare updated metadata
        updated_metadata = {
            META_TYPE: TYPE_MANUAL,
            META_KEY: key,
            META_CREATED_AT: existing_item.get("createdAt", now_iso), # Keep original creation time
            META_UPDATED_AT: now_iso,
            "name": key,  # Add name field to metadata with key as value
        }

        try:
            collection.update(
                ids=[key],
                documents=[description],
                metadatas=[updated_metadata]
            )
            logger.info(f"Updated manual info item with key: {key}, description: '{description[:50]}{'...' if len(description) > 50 else ''}'")
            return {
                "id": key,
                "key": key,
                "description": description,
                "createdAt": updated_metadata[META_CREATED_AT],
                "updatedAt": updated_metadata[META_UPDATED_AT]
            }
        except Exception as e:
            logger.error(f"Error updating manual info for key '{key}': {e}")
            return None # Indicate update failure

    def delete_manual_info(self, key: str) -> bool:
        """Deletes a manual information item by its key."""
        collection = self._get_manual_info_collection()
        try:
            # First get the item to log its description before deleting
            existing = self.get_manual_info(key)
            if not existing:
                logger.warning(f"Attempted to delete non-existent manual info item: {key}")
                return False

            collection.delete(ids=[key])
            logger.info(f"Deleted manual info item with key: {key}, description: '{existing['description'][:50]}{'...' if len(existing['description']) > 50 else ''}'")
            return True
        except Exception as e:
            logger.error(f"Error deleting manual info for key '{key}': {e}")
            return False

    def list_manual_info(self, page: int = 1, limit: int = 10, search: Optional[str] = None) -> Tuple[List[Dict[str, Any]], int]:
        """Lists manual information items with pagination and optional search."""
        collection = self._get_manual_info_collection()
        offset = (page - 1) * limit

        where_clause = {META_TYPE: TYPE_MANUAL}
        where_document_clause = None

        if search:
            # Simple search: check if search term is in key metadata OR description document
            # Note: This might be slow for large datasets without specific indexing.
            # Option 1: Search key metadata (exact match for now)
            # where_clause[META_KEY] = search
            # Option 2: Search document content
            where_document_clause = {"$contains": search}
            # Option 3: Combine? Could use $or in where_document if supported robustly
            # For now, let's prioritize document search for flexibility
            logger.debug(f"Applying search filter (in description): '{search}'")


        try:
            # First, get the total count matching the filter
            # Note: ChromaDB's count() doesn't directly support where_document.
            # We have to fetch all matching IDs/metadata first, then count. This isn't ideal for large datasets.
            # A more scalable approach might involve fetching only IDs/metadata and then counting.
            all_matching_items = collection.get(
                 where=where_clause,
                 where_document=where_document_clause,
                 include=[] # Don't need full data for count
            )
            total_count = len(all_matching_items['ids']) if all_matching_items and all_matching_items.get('ids') else 0
            logger.debug(f"Total count for manual info matching filter: {total_count}")


            # Then, get the paginated results
            results = collection.get(
                where=where_clause,
                where_document=where_document_clause, # Apply search filter here too
                limit=limit,
                offset=offset,
                include=['metadatas', 'documents']
            )

            items_data = []
            if results and results.get('ids') and results['ids']:
                for i in range(len(results['ids'])):
                    doc_id = results['ids'][i]
                    document = results['documents'][i] if results.get('documents') and i < len(results['documents']) else ""
                    metadata = results['metadatas'][i] if results.get('metadatas') and i < len(results['metadatas']) else {}

                    items_data.append({
                        "id": doc_id,
                        "key": metadata.get(META_KEY, doc_id),
                        "description": document,
                        "createdAt": metadata.get(META_CREATED_AT),
                        "updatedAt": metadata.get(META_UPDATED_AT)
                    })

            return items_data, total_count

        except Exception as e:
            logger.error(f"Error listing manual info: {e}")
            return [], 0

# Global vector store instance
vector_store = VectorStore()

File: rag/api/routes/auth.py
"""Authentication routes."""
import logging
from datetime import timedelta

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm

from rag.config import config
from rag.api.auth import (
    authenticate_user, create_access_token, get_current_active_user, 
    fake_users_db, create_user
)
from rag.api.models import Token, User, NewUserRequest

logger = logging.getLogger("auth_routes")

router = APIRouter(tags=["authentication"])

@router.post("/token", response_model=Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    """OAuth2 compatible token login, get an access token for future requests."""
    if not config.auth.enabled:
        # If auth is disabled, return a dummy token
        access_token = create_access_token(data={"sub": "admin"})
        return {"access_token": access_token, "token_type": "bearer"}
    
    user = authenticate_user(fake_users_db, form_data.username, form_data.password)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token_expires = timedelta(minutes=config.auth.token_expire_minutes)
    access_token = create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    return {"access_token": access_token, "token_type": "bearer"}

@router.post("/users", response_model=User)
async def create_new_user(user_request: NewUserRequest, current_user: User = Depends(get_current_active_user)):
    """Create a new user (admin only)."""
    if not config.auth.enabled:
        return {"message": "Authentication is disabled, user not created"}
    
    if current_user.username != config.auth.default_admin["username"]:
        raise HTTPException(status_code=403, detail="Only admin can create users")
    
    try:
        return create_user(
            username=user_request.username,
            password=user_request.password,
            disabled=user_request.disabled
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

@router.get("/users/me", response_model=User)
async def read_users_me(current_user: User = Depends(get_current_active_user)):
    """Get current user information."""
    return current_user

File: rag/api/auth.py
"""Authentication and authorization utilities."""
import logging
from datetime import datetime, timedelta
from typing import Dict, Optional, Union
from functools import wraps

from fastapi import Depends, FastAPI, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from passlib.context import CryptContext
from pydantic import BaseModel
# In auth.py, change:
from rag.api.models import Token, User, UserInDB  # Import from models

from rag.config import config

logger = logging.getLogger("auth")

# Security models
class TokenData(BaseModel):
    username: Optional[str] = None

class User(BaseModel):
    username: str
    disabled: Optional[bool] = None

class UserInDB(User):
    hashed_password: str

# Initialize security utilities
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token", auto_error=False)

# Mock user database - replace with a real database in production
fake_users_db = {
    config.auth.default_admin["username"]: {
        "username": config.auth.default_admin["username"],
        "hashed_password": pwd_context.hash(config.auth.default_admin["password"]),
        "disabled": False
    }
}

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """Verify a password against a hash."""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """Hash a password."""
    return pwd_context.hash(password)

def get_user(db: Dict, username: str) -> Optional[UserInDB]:
    """Get a user from the database."""
    if username in db:
        user_dict = db[username]
        return UserInDB(**user_dict)
    return None

def authenticate_user(db: Dict, username: str, password: str) -> Union[UserInDB, bool]:
    """Authenticate a user."""
    user = get_user(db, username)
    if not user:
        return False
    if not verify_password(password, user.hashed_password):
        return False
    return user

def create_access_token(data: Dict, expires_delta: Optional[timedelta] = None) -> str:
    """Create a JWT access token."""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, config.auth.secret_key, algorithm="HS256")

async def get_current_user(token: str = Depends(oauth2_scheme)) -> User:
    """Get the current user from token."""
    # If auth is disabled, return a default admin user
    if not config.auth.enabled:
        return User(username="admin", disabled=False)

    # No token provided
    if not token:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Not authenticated",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, config.auth.secret_key, algorithms=["HS256"])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
        token_data = TokenData(username=username)
    except JWTError:
        raise credentials_exception
    user = get_user(fake_users_db, username=token_data.username)
    if user is None:
        raise credentials_exception
    return user

async def get_current_active_user(current_user: User = Depends(get_current_user)) -> User:
    """Get the current active user."""
    if current_user.disabled:
        raise HTTPException(status_code=400, detail="Inactive user")
    return current_user

def create_user(username: str, password: str, disabled: bool = False) -> User:
    """Create a new user."""
    if username in fake_users_db:
        raise ValueError(f"User {username} already exists")
    
    fake_users_db[username] = {
        "username": username,
        "hashed_password": get_password_hash(password),
        "disabled": disabled
    }
    
    return User(username=username, disabled=disabled)

def conditional_auth(func):
    """Decorator to conditionally apply authentication."""
    @wraps(func)
    async def wrapper(*args, current_user: User = Depends(get_current_active_user), **kwargs):
        if config.auth.enabled:
            # current_user will be filled by the dependency
            return await func(*args, current_user=current_user, **kwargs)
        else:
            # Auth is disabled, call without current_user
            return await func(*args, **kwargs)
    return wrapper

File: rag/api/routes/chat.py
# rag/api/routes/chat.py
import asyncio
import logging
import json # For NDJSON
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from rag.api.models import GenAIChatRequest, GenAIChatResponseChunk, ChatMessagePy

logger = logging.getLogger("chat_routes")
router = APIRouter(tags=["Chat"])

# NEW: Minimal stream generator for testing
async def generate_minimal_stream_for_debug():
    try:
        chunk1_data = {"text": "DebugChunk1 ", "done": False, "error": None}
        chunk1_str = json.dumps(chunk1_data) + "\\n"
        logger.info(f"Yielding DEBUG chunk 1: {chunk1_str!r}")
        yield chunk1_str
        await asyncio.sleep(0.1)

        chunk2_data = {"text": "DebugChunk2", "done": True, "error": None}
        chunk2_str = json.dumps(chunk2_data) + "\\n"
        logger.info(f"Yielding DEBUG chunk 2: {chunk2_str!r}")
        yield chunk2_str
    except Exception as e:
        logger.error(f"Error in generate_minimal_stream_for_debug: {e}", exc_info=True)
        error_chunk_data = {"text": "", "done": True, "error": f"Error in debug stream: {e}"}
        yield json.dumps(error_chunk_data) + "\\n"

async def generate_simulated_llm_stream(messages: list[ChatMessagePy], model: str | None):
    """
    Simulates an LLM generating a response by streaming words from the last user message.
    Each word is a chunk in NDJSON format.
    """
    logger.info(f"Simulating LLM response for model: {model or 'default'}")
    
    last_user_message_content = "No user message found."
    for msg in reversed(messages):
        if msg.role == "user":
            last_user_message_content = msg.content
            break

    response_prefix = f"Model \'{model or 'default'}\' responding to: "
    full_simulated_response = response_prefix + last_user_message_content
    
    words = full_simulated_response.split()

    if not words: # Handle empty message
        try:
            chunk_data = GenAIChatResponseChunk(text="", done=True)
            json_chunk_str = json.dumps(chunk_data.model_dump()) + "\\n"
            logger.info(f"Yielding empty message chunk: {json_chunk_str!r}")
            yield json_chunk_str
            return
        except Exception as e:
            logger.error(f"Error yielding empty message chunk: {e}", exc_info=True)
            # Fallback error chunk
            error_chunk_data = {"text": "", "done": True, "error": f"Error in empty message: {e}"}
            yield json.dumps(error_chunk_data) + "\\n"
            return


    for i, word in enumerate(words):
        is_done = i == len(words) - 1
        try:
            chunk_data = GenAIChatResponseChunk(text=word + " ", done=is_done)
            # Convert Pydantic model to dict, then to JSON string, then add newline for NDJSON
            json_chunk_str = json.dumps(chunk_data.model_dump()) + "\\n" # Use model_dump() for Pydantic v2
            logger.info(f"Yielding chunk ({i+1}/{len(words)}): {json_chunk_str!r}")
            yield json_chunk_str
            await asyncio.sleep(0.1) # Simulate delay
        except Exception as e:
            logger.error(f"Error yielding chunk {i+1}: {e}", exc_info=True)
            # Attempt to yield a final error chunk if loop iteration fails
            error_chunk_data = {"text": "", "done": True, "error": f"Error in word chunk {i+1}: {e}"}
            yield json.dumps(error_chunk_data) + "\\n"
            return # Stop further processing

    # This final "done" message might be redundant if the loop handles the last chunk correctly with done=True
    # However, it ensures a "done" signal if words list was empty and the initial check didn't catch it (though it should)
    # or if an error occurred and we want to signal termination.
    # For now, the loop's last chunk should set done=True.
    # Consider if a final explicit done is needed if loop is empty AFTER the initial check.
    # if not words: # This condition might be re-evaluated based on above logic.
    #     final_done_chunk = GenAIChatResponseChunk(text="", done=True)
    #     logger.info(f"Yielding final explicit done chunk: {json.dumps(final_done_chunk.model_dump()) + '\\n'!r}")
    #     yield json.dumps(final_done_chunk.model_dump()) + "\\n"


@router.post("/v1/chat/completions", summary="Chat Completions Endpoint")
async def chat_completions(
    request: GenAIChatRequest
):
    """
    Handles chat completion requests.
    Accepts a list of messages and streams back responses.
    """
    logger.info(f"Received chat completion request. Model: {request.model or 'default'}, Messages: {len(request.messages)}")

    if not request.messages:
        raise HTTPException(status_code=400, detail="No messages provided")

    if request.stream:
        # logger.info("Streaming response (using generate_simulated_llm_stream)...") 
        logger.info("Streaming response (using generate_minimal_stream_for_debug)...") # MODIFIED
        return StreamingResponse(
            # generate_simulated_llm_stream(request.messages, request.model), # COMMENTED OUT
            generate_minimal_stream_for_debug(), # USING MINIMAL DEBUG STREAM
            media_type="application/x-ndjson" # Newline Delimited JSON
        )
    else:
        # Non-streaming response
        logger.info("Generating non-streaming response...")
        full_response_text = ""
        # This non-streaming path needs to be robust if used.
        # For now, it reuses the async generator, which is fine.
        async for chunk_json_str in generate_simulated_llm_stream(request.messages, request.model):
            try:
                # Assuming chunk_json_str includes the newline, so strip it before parsing
                chunk_dict = json.loads(chunk_json_str.strip())
                if chunk_dict.get("text"):
                    full_response_text += chunk_dict["text"]
                if chunk_dict.get("done"):
                    break
            except json.JSONDecodeError as e:
                logger.error(f"Failed to decode chunk for non-streaming: {chunk_json_str!r}, Error: {e}")
                # Decide how to handle, maybe append raw or log and continue
        
        final_chunk = GenAIChatResponseChunk(text=full_response_text, done=True)
        return final_chunk


File: rag/api/routes/__init__.py
# Routes package 


File: rag/api/routes/items.py
"""Item management routes."""
import logging

from fastapi import APIRouter, HTTPException, Depends

from rag.db.vector_store import vector_store
from rag.api.auth import conditional_auth, User
from rag.api.models import BatchAddRequest

logger = logging.getLogger("item_routes")

router = APIRouter(tags=["items"])

from pydantic import ValidationError

@router.post("/add_batch")
async def add_batch(request: dict):
    try:
        # Try to validate the request manually
        batch_request = BatchAddRequest(**request)
        
        # If validation passes, proceed
        count = vector_store.add_items(
            batch_request.collection_name,
            [item.dict() for item in batch_request.items]
        )
        
        return {
            "message": f"Successfully added {len(batch_request.items)} items to collection {batch_request.collection_name}",
            "count": len(batch_request.items)
        }
    except ValidationError as e:
        # Log the validation error details
        logger.error(f"Validation error: {e.json()}")
        raise HTTPException(status_code=422, detail=e.errors())
    except Exception as e:
        logger.error(f"Error adding batch: {e}")
        raise HTTPException(status_code=500, detail=f"Error adding batch: {str(e)}")

File: rag/api/routes/rag_info.py
# rag/api/routes/rag_info.py
"""Routes for managing manual RAG information."""
import logging
import math
from datetime import datetime, timezone
from typing import Optional # Import Optional

from fastapi import APIRouter, HTTPException, Depends, Query, Path, Body, status

# Assuming vector_store is correctly initialized in rag/db/vector_store.py
from rag.db.vector_store import vector_store
# Assuming auth utilities are correctly defined
from rag.api.auth import User, get_current_active_user
from rag.api.models import (
    RagInfoItem,
    RagInfoItemCreate,
    RagInfoItemUpdate,
    RagInfoPageResponse # Use the correct Pydantic model for the list response
)
# Assuming config is correctly loaded
from rag.config import config

logger = logging.getLogger("rag_info_routes")

# Define a placeholder dependency that does nothing when auth is disabled
# It MUST be an async function if the real dependency is also async
async def get_no_auth_dependency() -> None:
    return None

# REMOVED prefix from router definition
router = APIRouter(tags=["RAG Information"])

DEFAULT_PAGE_SIZE = 10 # Consistent page size

@router.get(
    "",
    response_model=RagInfoPageResponse, # Use the correct response model
    summary="List RAG Information Items",
    description="Retrieves a paginated list of manually added RAG information items, with optional search.",
)
async def list_rag_info(
    page: int = Query(1, ge=1, description="Page number (1-based)"),
    limit: int = Query(DEFAULT_PAGE_SIZE, ge=1, le=100, description="Items per page"),
    search: Optional[str] = Query(None, description="Search term for key or description"),
    _user: Optional[User] = Depends(get_current_active_user if config.auth.enabled else get_no_auth_dependency)
):
    """
    Lists manually added RAG information entries with pagination and search.
    Matches the GET /v1/rag-info endpoint expected by the Go client.
    """
    logger.info(f"Listing RAG info: page={page}, limit={limit}, search='{search or ''}'")
    try:
        # vector_store.list_manual_info returns a tuple: (list_of_dicts, total_count)
        items_data, total_count = vector_store.list_manual_info(page=page, limit=limit, search=search)

        items_list = []
        for item_dict in items_data:
             try:
                 # Attempt to parse dates, provide defaults if missing/invalid
                 # Chroma metadata values are often strings, ensure robust parsing
                 created_at_str = item_dict.get('createdAt')
                 updated_at_str = item_dict.get('updatedAt')

                 created_at = datetime.fromisoformat(created_at_str) if created_at_str else datetime.now(timezone.utc)
                 updated_at = datetime.fromisoformat(updated_at_str) if updated_at_str else datetime.now(timezone.utc)

                 items_list.append(RagInfoItem(
                     id=str(item_dict.get('id', 'N/A')), # Ensure ID is string
                     key=str(item_dict.get('key', 'N/A')), # Ensure key is string
                     description=str(item_dict.get('description', '')),
                     createdAt=created_at,
                     updatedAt=updated_at
                 ))
             except (ValueError, TypeError) as date_err:
                  logger.warning(f"Could not parse date for item {item_dict.get('id')}: {date_err}. Using current time.")
                  # Append with current time as fallback, ensure types match model
                  items_list.append(RagInfoItem(
                     id=str(item_dict.get('id', 'parse_error_id')),
                     key=str(item_dict.get('key', 'parse_error_key')),
                     description=str(item_dict.get('description', '')),
                     createdAt=datetime.now(timezone.utc),
                     updatedAt=datetime.now(timezone.utc)
                  ))

        total_pages = math.ceil(total_count / limit) if limit > 0 else 1

        return RagInfoPageResponse(
            items=items_list,
            totalCount=total_count,
            totalPages=total_pages,
            currentPage=page
        )
    except Exception as e:
        logger.error(f"Error listing RAG info: {e}", exc_info=True) # Log stack trace
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve RAG information list."
        )

@router.post(
    "",
    response_model=RagInfoItem,
    status_code=status.HTTP_201_CREATED,
    summary="Create RAG Information Item",
    description="Adds a new key-value information item to the RAG store.",
)
async def create_rag_info(
    item_in: RagInfoItemCreate,
    _user: Optional[User] = Depends(get_current_active_user if config.auth.enabled else get_no_auth_dependency)
):
    """
    Creates a new manual RAG information entry.
    Matches the POST /v1/rag-info endpoint.
    """
    user_info = _user.username if config.auth.enabled and _user else 'anonymous'
    logger.info(f"User '{user_info}' creating RAG info item with key: {item_in.key}, description: '{item_in.description[:50]}{'...' if len(item_in.description) > 50 else ''}'")
    try:
        # vector_store.add_manual_info should return a dict matching RagInfoItem structure
        created_item_dict = vector_store.add_manual_info(key=item_in.key, description=item_in.description)

        # Parse dates from the returned dict
        created_at = datetime.fromisoformat(created_item_dict.get('createdAt'))
        updated_at = datetime.fromisoformat(created_item_dict.get('updatedAt'))

        return RagInfoItem(
             id=str(created_item_dict.get('id')),
             key=str(created_item_dict.get('key')),
             description=str(created_item_dict.get('description')),
             createdAt=created_at,
             updatedAt=updated_at
         )
    except HTTPException as http_exc: # Catch potential 409 from vector store add
         logger.warning(f"HTTP Exception during RAG info creation for key '{item_in.key}': {http_exc.detail}")
         raise http_exc
    except Exception as e:
        logger.error(f"Error creating RAG info for key '{item_in.key}': {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to create RAG information item."
        )

@router.get(
    "/{item_id}",
    response_model=RagInfoItem,
    summary="Get RAG Information Item",
    description="Retrieves a specific RAG information item by its key (ID).",
)
async def get_rag_info(
    item_id: str = Path(..., description="The key/ID of the RAG info item to retrieve"),
    _user: Optional[User] = Depends(get_current_active_user if config.auth.enabled else get_no_auth_dependency)
):
    """
    Retrieves a specific manual RAG information item by its key.
    Matches the GET /v1/rag-info/{id} endpoint.
    """
    logger.debug(f"Attempting to retrieve RAG info item with key: {item_id}")
    try:
        item_dict = vector_store.get_manual_info(key=item_id)
        if item_dict is None:
            logger.warning(f"RAG info item not found: {item_id}")
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="RAG info item not found")

        # Parse dates
        created_at_str = item_dict.get('createdAt')
        updated_at_str = item_dict.get('updatedAt')
        created_at = datetime.fromisoformat(created_at_str) if created_at_str else datetime.now(timezone.utc)
        updated_at = datetime.fromisoformat(updated_at_str) if updated_at_str else datetime.now(timezone.utc)

        return RagInfoItem(
            id=str(item_dict.get('id')),
            key=str(item_dict.get('key')),
            description=str(item_dict.get('description')),
            createdAt=created_at,
            updatedAt=updated_at
        )
    except HTTPException:
         raise # Re-raise HTTP 404
    except (ValueError, TypeError) as date_err:
        logger.error(f"Error parsing date for RAG info item '{item_id}': {date_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to parse date format for the item.")
    except Exception as e:
        logger.error(f"Error retrieving RAG info for key '{item_id}': {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve RAG information item."
        )

@router.put(
    "/{item_id}",
    response_model=RagInfoItem,
    summary="Update RAG Information Item",
    description="Updates the description of an existing RAG information item.",
)
async def update_rag_info(
    item_id: str = Path(..., description="The key/ID of the item to update"),
    item_update: RagInfoItemUpdate = Body(...),
    _user: Optional[User] = Depends(get_current_active_user if config.auth.enabled else get_no_auth_dependency)
):
    """
    Updates the description of a specific manual RAG information item.
    Matches the PUT /v1/rag-info/{id} endpoint.
    """
    user_info = _user.username if config.auth.enabled and _user else 'anonymous'
    logger.info(f"User '{user_info}' updating RAG info item: {item_id}, description: '{item_update.description[:50]}{'...' if len(item_update.description) > 50 else ''}'")
    try:
        # vector_store.update_manual_info should return a dict or None
        updated_item_dict = vector_store.update_manual_info(key=item_id, description=item_update.description)
        if updated_item_dict is None:
            logger.warning(f"Attempted to update non-existent RAG info item: {item_id}")
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="RAG info item not found")

        # Parse dates
        created_at_str = updated_item_dict.get('createdAt')
        updated_at_str = updated_item_dict.get('updatedAt')
        created_at = datetime.fromisoformat(created_at_str) if created_at_str else datetime.now(timezone.utc)
        updated_at = datetime.fromisoformat(updated_at_str) if updated_at_str else datetime.now(timezone.utc)

        return RagInfoItem(
            id=str(updated_item_dict.get('id')),
            key=str(updated_item_dict.get('key')),
            description=str(updated_item_dict.get('description')),
            createdAt=created_at,
            updatedAt=updated_at
        )
    except HTTPException:
         raise # Re-raise HTTP 404
    except (ValueError, TypeError) as date_err:
        logger.error(f"Error parsing date for updated RAG info item '{item_id}': {date_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to parse date format for the updated item.")
    except Exception as e:
        logger.error(f"Error updating RAG info for key '{item_id}': {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to update RAG information item."
        )

@router.delete(
    "/{item_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete RAG Information Item",
    description="Deletes a specific RAG information item by its key (ID).",
)
async def delete_rag_info(
    item_id: str = Path(..., description="The key/ID of the item to delete"),
    _user: Optional[User] = Depends(get_current_active_user if config.auth.enabled else get_no_auth_dependency)
):
    """
    Deletes a specific manual RAG information item by its key.
    Matches the DELETE /v1/rag-info/{id} endpoint.
    """
    user_info = _user.username if config.auth.enabled and _user else 'anonymous'
    logger.info(f"User '{user_info}' deleting RAG info item: {item_id}")
    try:
        # vector_store.delete_manual_info returns bool
        deleted = vector_store.delete_manual_info(key=item_id)
        if not deleted:
            # To provide accurate 404, check if it existed right before trying to delete
            # (There's a small race condition window here, but generally okay)
            if vector_store.get_manual_info(key=item_id) is None:
                 logger.warning(f"Attempted to delete RAG info item that was not found: {item_id}")
                 raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="RAG info item not found")
            else:
                 # If it exists but delete failed, it's likely an internal error
                 logger.error(f"Delete operation failed unexpectedly for RAG info item: {item_id}")
                 raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to delete RAG information item.")
        # If deleted is True, FastAPI automatically returns 204 No Content
        return None
    except HTTPException:
        raise # Re-raise HTTP 404 or others
    except Exception as e:
        logger.error(f"Error deleting RAG info for key '{item_id}': {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete RAG information item."
        )

File: rag/api/routes/search.py
"""Search routes."""
import logging

from fastapi import APIRouter, HTTPException, Query

from rag.db.vector_store import vector_store
from rag.api.models import SimilarityResponse, SimilarityResult, MultiCollectionSearchResponse

logger = logging.getLogger("search_routes")

router = APIRouter(tags=["search"])

@router.get("/search", response_model=SimilarityResponse)
async def search_similar(
    query: str = Query(..., description="Description to search for"),
    collection_name: str = Query(..., description="Collection name"),
    limit: int = Query(5, description="Number of results to return")
):
    """Search for similar items in the specified ChromaDB collection."""
    logger.info(f"Received search request for query: '{query}' in collection: '{collection_name}', limit: {limit}")
    
    try:
        # Validate input
        if not query:
            raise HTTPException(status_code=400, detail="Query is required")
        
        if not collection_name:
            raise HTTPException(status_code=400, detail="Collection name is required")
        
        # Get search results
        results = vector_store.search(
            collection_name=collection_name,
            query=query,
            limit=limit
        )
        
        # Log search results details
        result_count = len(results)
        logger.info(f"Search completed for '{query}' in '{collection_name}'. Found {result_count} results.")
        
        # Log a preview of top result if available
        if result_count > 0:
            top_result = results[0]
            # Include description in the log if it exists
            description = top_result.get('description', '')
            description_preview = f", description: '{description[:50]}{'...' if len(description) > 50 else ''}'" if description else ""
            logger.info(f"Top result for '{query}': code={top_result.get('code')}, name='{top_result.get('name')}', similarity={top_result.get('similarity_score', 0):.3f}{description_preview}")
        
        # Convert to API response model
        similarity_results = [SimilarityResult(**result) for result in results]
        
        return {
            "query": query,
            "collection_name": collection_name,
            "results": similarity_results
        }
    except Exception as e:
        logger.error(f"Search error for query '{query}' in collection '{collection_name}': {e}")
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

@router.get("/search_all", response_model=MultiCollectionSearchResponse)
async def search_across_collections(
    query: str = Query(..., description="Description to search for"),
    limit_per_collection: int = Query(3, description="Number of results per collection"),
    min_score: float = Query(0.0, description="Minimum similarity score (0-1)")
):
    """Search for similar items across all collections."""
    logger.info(f"Received search_all request for query: '{query}', limit_per_collection: {limit_per_collection}, min_score: {min_score}")
    
    try:
        # Validate input
        if not query:
            raise HTTPException(status_code=400, detail="Query is required")
        
        # Get search results across all collections
        all_results = vector_store.search_all_collections(
            query=query,
            limit_per_collection=limit_per_collection,
            min_score=min_score
        )
        
        # Log results summary
        collection_counts = {coll: len(items) for coll, items in all_results.items()}
        total_items = sum(len(items) for items in all_results.values())
        logger.info(f"Search_all completed for '{query}'. Found {total_items} results across {len(all_results)} collections: {collection_counts}")
        
        # Log top result from each collection if available
        for coll_name, items in all_results.items():
            if items:
                top_result = items[0]
                logger.info(f"Top result in '{coll_name}' for '{query}': code={top_result.get('code')}, name='{top_result.get('name')}', similarity={top_result.get('similarity_score', 0):.3f}")
        
        # Convert to API response model
        formatted_results = {
            collection: [SimilarityResult(**item) for item in items]
            for collection, items in all_results.items()
        }
        
        return {
            "query": query,
            "results": formatted_results
        }
    except Exception as e:
        logger.error(f"Search_all error for query '{query}': {e}")
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

File: rag/api/app.py
# rag/api/app.py (Modified)
"""FastAPI application initialization."""
import logging
import asyncio
import time

from fastapi import FastAPI, HTTPException, Depends # Keep existing FastAPI imports
from fastapi.middleware.cors import CORSMiddleware # Keep existing

from rag.config import config # Your existing config
from rag.db.vector_store import vector_store # Your existing vector_store
from rag.db.postgres_reader import fetch_unspsc_commodities, engine as pg_engine # Your existing db stuff
from rag.api.routes import auth, collections, items, search, rag_info # Your existing RAG API routes
from rag.api.models import StatusResponse 

# --- Import LangGraph Visualization Routers ---
from rag.langgraph_vis import api_routes as langgraph_api_router
# from rag.langgraph_vis import ws_handler as langgraph_ws_router # Commented out WebSocket router
from rag.langgraph_vis import sse_handler as langgraph_sse_router # NEW: Import SSE router

logger = logging.getLogger("app") # Assuming "app" is your root logger for this file

# --- Startup Event Function (Combined Check & Populate) ---
# Your existing startup_event function (ensure it's still relevant and works)
async def startup_event():
    """Ensure required ChromaDB collections exist and populate UNSPSC if needed."""
    logger.info("Running startup tasks...")
    start_time_overall = time.time()
    loop = asyncio.get_running_loop()
    wait_time = 5
    logger.info(f"Waiting {wait_time} seconds for ChromaDB service to potentially initialize...")
    await asyncio.sleep(wait_time)    
    required_collections = [
        config.chromadb.manual_info_collection,
        config.chromadb.unspsc_collection,
        config.chromadb.common_collection
    ]
    logger.info(f"Ensuring ChromaDB collections exist: {required_collections}")
    collections_ok = True
    for i, col_name in enumerate(required_collections):
        try:
            await loop.run_in_executor(None, vector_store.get_collection, col_name, True)
            logger.info(f"Collection '{col_name}' ensured.")
        except Exception as e:
            logger.error(f"CRITICAL: Failed to get or create collection '{col_name}': {e}", exc_info=True)
            collections_ok = False
        # Yield control after each collection check to prevent blocking
        if i % 1 == 0:
            await asyncio.sleep(0)
    if not collections_ok:
        logger.error("Aborting further startup tasks due to collection creation errors.")
        return

    unspsc_collection_name = config.chromadb.unspsc_collection
    logger.info(f"Checking population status for '{unspsc_collection_name}'...")
    try:
        unspsc_collection = await loop.run_in_executor(None, vector_store.get_collection, unspsc_collection_name, False)
        count = await loop.run_in_executor(None, unspsc_collection.count)
        logger.info(f"Collection '{unspsc_collection_name}' current item count: {count}")
        if count == 0:
            logger.info(f"Collection '{unspsc_collection_name}' is empty. Attempting to populate from PostgreSQL...")
            populate_start_time = time.time()
            logger.info("Fetching UNSPSC data from PostgreSQL...")
            if pg_engine is None:
                 logger.error("PostgreSQL engine not initialized. Skipping population.")
                 unspsc_items = []
            else:
                unspsc_items = await loop.run_in_executor(None, fetch_unspsc_commodities)
                logger.info(f"Fetched {len(unspsc_items)} UNSPSC commodities from PostgreSQL.")

            if not unspsc_items:
                logger.warning(f"No UNSPSC commodity items fetched. Collection '{unspsc_collection_name}' will remain empty.")
            else:
                logger.info(f"Preparing and adding {len(unspsc_items)} items in batches...")
                ids = [item["code"] for item in unspsc_items]
                documents = [item.get("description") or item.get("name", "") for item in unspsc_items]
                metadatas = [{"code": item["code"], "name": item["name"], "item_type": "unspsc_commodity"} for item in unspsc_items]
                batch_size = 500
                added_count = 0
                total_batches = (len(ids) + batch_size - 1) // batch_size
                batch_start_time = time.time()
                for i in range(0, len(ids), batch_size):
                    batch_num = i // batch_size + 1
                    logger.info(f"Processing batch {batch_num}/{total_batches}...")
                    batch_ids, batch_docs, batch_metas = ids[i:i+batch_size], documents[i:i+batch_size], metadatas[i:i+batch_size]
                    try:
                        await loop.run_in_executor(None, lambda: unspsc_collection.add(ids=batch_ids, documents=batch_docs, metadatas=batch_metas))        # After populating each batch, yield control to let other tasks run
                        added_count += len(batch_ids)
                        current_time = time.time()
                        logger.info(f"Added batch {batch_num}/{total_batches} ({len(batch_ids)} items). Total added: {added_count}. Batch took: {current_time - batch_start_time:.2f}s")
                        batch_start_time = current_time
                        # Yield to the event loop periodically during large ingestions
                        if batch_num % 5 == 0:  # Every 5 batches
                            logger.debug(f"Yielding event loop during UNSPSC population after batch {batch_num}")
                            await asyncio.sleep(0)
                    except Exception as batch_e:
                         logger.error(f"Error adding batch {batch_num} to '{unspsc_collection_name}': {batch_e}", exc_info=True)
                         logger.warning("Stopping population due to batch error.")
                         break
                final_count = await loop.run_in_executor(None, unspsc_collection.count)
                populate_duration = time.time() - populate_start_time
                logger.info(f"Finished populating '{unspsc_collection_name}'. Total items added: {added_count}, Final count: {final_count}, Duration: {populate_duration:.2f}s")
        else:
            logger.info(f"Collection '{unspsc_collection_name}' already contains data ({count} items). Skipping population.")
    except Exception as e:
        logger.error(f"Error during check/population of collection '{unspsc_collection_name}': {e}", exc_info=True)
    overall_duration = time.time() - start_time_overall
    logger.info(f"Startup tasks completed in {overall_duration:.2f} seconds.")


def create_app() -> FastAPI:
    """Create and configure the FastAPI application."""
    app_instance = FastAPI( # Renamed to app_instance to avoid conflict if 'app' is used globally later
        title="RAG Classification Support and LangGraph Visualization API", # MODIFIED title
        description="API for RAG, and visualizing/executing LangGraph workflows.", # MODIFIED description
        version="1.2.0", # MODIFIED version (example)
    )

    # Add CORS middleware (your existing setup)
    app_instance.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Add health check endpoint (your existing setup)
    @app_instance.get("/health")
    async def health_check():
        return {"status": "ok"}

    # Status endpoint (your existing setup, slightly adapted)
    @app_instance.get("/", response_model=StatusResponse, tags=["status"])
    async def get_status():
        try:
            chroma_connected = await asyncio.get_running_loop().run_in_executor(None, vector_store.test_connection)
            collections_list = [] # Renamed from 'collections' to avoid confusion
            if chroma_connected:
                try:
                    collections_info = await asyncio.get_running_loop().run_in_executor(None, vector_store.list_collections)
                    collections_list = [c["name"] for c in collections_info]
                except Exception as e:
                    logger.error(f"Error listing collections during status check: {e}")
            return {
                "status": "ok",
                "chroma_connected": chroma_connected,
                "collections": collections_list, # Ensure this matches StatusResponse model
                "auth_enabled": config.auth.enabled
            }
        except Exception as e:
            logger.error(f"Status check error: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"API status check error: {str(e)}")

    # Include existing RAG routers
    app_instance.include_router(auth.router, prefix="/v1/auth", tags=["RAG Authentication"]) # Example: Add prefix for clarity
    app_instance.include_router(collections.router, prefix="/v1/rag", tags=["RAG Collections"])
    app_instance.include_router(items.router, prefix="/v1/rag", tags=["RAG Items"])
    app_instance.include_router(search.router, prefix="/v1/rag", tags=["RAG Search"])
    app_instance.include_router(rag_info.router, prefix="/v1/rag-info", tags=["RAG Information"]) # This one already had a prefix

    # --- NEW: Include LangGraph Visualization Routers ---
    # You might want a common prefix for all LangGraph visualization endpoints
    LG_VIS_PREFIX = "/v1/lg-vis"
    app_instance.include_router(langgraph_api_router.router, prefix=LG_VIS_PREFIX, tags=["LangGraph Management"])
    # app_instance.include_router(langgraph_ws_router.router, prefix=LG_VIS_PREFIX, tags=["LangGraph Execution (WS)"]) # WebSocket router
    app_instance.include_router(langgraph_sse_router.router, prefix=LG_VIS_PREFIX, tags=["LangGraph Execution (SSE)"]) # NEW: Include SSE router
    # Note: The WebSocket paths in ws_handler.py already include "/ws/langgraph/graphs/..."
    # If you add a prefix here like "/v1/lg-vis", the full WebSocket path would become, e.g.,
    # "/v1/lg-vis/ws/langgraph/graphs/{graph_id}/execute". Ensure client-side reflects this.
    # Alternatively, don't prefix the WebSocket router here if its paths are already absolute.
    # Given the paths in ws_handler.py, it's probably better NOT to prefix the ws_router here if those paths are intended to be root-relative.
    # Let's re-evaluate ws_router paths:
    # If ws_handler.py has paths like "/ws/langgraph/graphs/...", then including it with a prefix here
    # would result in "/v1/lg-vis/ws/langgraph/graphs/...".
    # Let's assume the paths in ws_handler.py are intended to be relative to the prefix IF the router is prefixed.
    # A common practice for WebSockets is to have them at a distinct root path.
    # I'll keep the prefix for now, meaning the full WS URL will be prefixed. Adjust if needed.
    # --- END NEW ---

    # Register Startup Event (your existing setup)
    @app_instance.on_event("startup")
    async def on_startup():
        logger.info("Scheduling startup tasks (incl. potential population) to run in background.")
        asyncio.create_task(startup_event())

    logger.info(f"FastAPI app created. Auth enabled: {config.auth.enabled}. LangGraph Vis modules included.")
    return app_instance

# Create the FastAPI app instance
app = create_app() # This is the 'app' your uvicorn main.py likely runs

# If your rag/main.py uses `from . import app` (referring to rag/__init__.py which exports rag.api.app:app),
# ensure that this 'app' variable is correctly exposed.

File: rag/api/routes/collections.py
"""Collection management routes."""
import logging

from fastapi import APIRouter, HTTPException, Depends, Path

from rag.db.vector_store import vector_store
from rag.api.auth import conditional_auth, User
from rag.api.models import ListCollectionsResponse, CollectionInfo

logger = logging.getLogger("collection_routes")

router = APIRouter(tags=["collections"])

@router.get("/collections", response_model=ListCollectionsResponse)
async def list_collections():
    """List all available collections with their item counts."""
    try:
        collections = vector_store.list_collections()
        return {"collections": [CollectionInfo(**c) for c in collections]}
    except Exception as e:
        logger.error(f"Error listing collections: {e}")
        raise HTTPException(status_code=500, detail=f"Error listing collections: {str(e)}")

@router.post("/collection/{collection_name}")
@conditional_auth
async def create_collection(
    collection_name: str = Path(..., description="Collection name"),
    current_user: User = None
):
    """Create a new empty collection."""
    try:
        vector_store.get_collection(collection_name)
        return {"message": f"Collection {collection_name} created successfully"}
    except Exception as e:
        logger.error(f"Error creating collection: {e}")
        raise HTTPException(status_code=500, detail=f"Error creating collection: {str(e)}")

@router.delete("/collection/{collection_name}")
@conditional_auth
async def delete_collection(
    collection_name: str = Path(..., description="Collection name"),
    current_user: User = None
):
    """Delete a collection from ChromaDB."""
    try:
        if vector_store.delete_collection(collection_name):
            return {"message": f"Collection {collection_name} deleted successfully"}
        else:
            raise HTTPException(status_code=500, detail=f"Failed to delete collection {collection_name}")
    except Exception as e:
        logger.error(f"Error deleting collection: {e}")
        raise HTTPException(status_code=500, detail=f"Error deleting collection: {str(e)}")

File: client.py
"""Client for the Classification API."""
import requests
import json
from typing import List, Dict, Optional

class ClassificationClient:
    """Client for interacting with the Classification API."""
    
    def __init__(
        self, 
        base_url: str = None,
        api_key: str = None,
        headers: Dict[str, str] = None,
        verify_ssl: bool = True
    ):
        """Initialize the RAG Client with custom settings.
        
        Args:
            base_url: Base URL of the API (e.g., 'http://localhost:8090')
            api_key: Optional API key for authentication
            headers: Optional additional headers to send with each request
            verify_ssl: Whether to verify SSL certificates. Set to False for self-signed certs.
        """
        self.base_url = base_url.rstrip('/')
        self.username = None
        self.password = None
        self.token = None
        self.auth_enabled = True
        
        # Check if authentication is enabled
        self._check_auth_status()
        
        # Get token if authentication is enabled and credentials are provided
        if self.auth_enabled and self.username and self.password:
            self._get_token()
    
    def _check_auth_status(self):
        """Check if authentication is enabled on the server."""
        try:
            status = self.check_status()
            self.auth_enabled = status.get("auth_enabled", True)
        except Exception as e:
            # Assume authentication is enabled if we can't determine
            self.auth_enabled = True
    
    def _get_token(self):
        """Get OAuth2 token."""
        if not self.auth_enabled:
            return
        
        if not self.username or not self.password:
            raise ValueError("Username and password required for authentication")
        
        url = f"{self.base_url}/token"
        data = {
            "username": self.username,
            "password": self.password
        }
        
        response = requests.post(url, data=data)
        if response.status_code == 200:
            self.token = response.json().get("access_token")
        else:
            raise Exception(f"Failed to get token: {response.text}")
    
    def _get_headers(self):
        """Get headers for API requests."""
        headers = {
            "Content-Type": "application/json"
        }
        
        if self.auth_enabled and self.token:
            headers["Authorization"] = f"Bearer {self.token}"
        
        return headers
    
    def check_status(self):
        """Check API status."""
        url = f"{self.base_url}/"
        response = requests.get(url)
        return response.json()
    
    def list_collections(self):
        """List all available collections."""
        url = f"{self.base_url}/collections"
        response = requests.get(url)
        return response.json()
    
    def create_collection(self, collection_name: str):
        """Create a new collection.
        
        Args:
            collection_name: Name of the collection to create
            
        Returns:
            API response
        """
        url = f"{self.base_url}/collection/{collection_name}"
        response = requests.post(url, headers=self._get_headers())
        
        if response.status_code == 401 and self.auth_enabled:
            # If token might be expired, try to refresh it
            self._get_token()
            response = requests.post(url, headers=self._get_headers())
        
        return response.json()
    
    def add_batch(self, items: List[Dict], collection_name: str):
        """Add a batch of items to the collection.
        
        Args:
            items: List of items with code, name, description, etc.
            collection_name: Collection name
        
        Returns:
            API response
        """
        url = f"{self.base_url}/add_batch"
        data = {
            "items": items,
            "collection_name": collection_name
        }
        
        response = requests.post(url, json=data, headers=self._get_headers())
        
        if response.status_code == 401 and self.auth_enabled:
            # If token might be expired, try to refresh it
            self._get_token()
            response = requests.post(url, json=data, headers=self._get_headers())
        
        return response.json()
    
    def search(self, query: str, collection_name: str, limit: int = 5):
        """Search for similar items in a specific collection.
        
        Args:
            query: Text query to search for
            collection_name: Collection name
            limit: Maximum number of results to return
        
        Returns:
            Search results
        """
        url = f"{self.base_url}/search"
        params = {
            "query": query,
            "collection_name": collection_name,
            "limit": limit
        }
        
        response = requests.get(url, params=params)
        return response.json()
    
    def search_all(self, query: str, limit_per_collection: int = 3, min_score: float = 0.0):
        """Search for similar items across all collections.
        
        Args:
            query: Text query to search for
            limit_per_collection: Maximum number of results per collection
            min_score: Minimum similarity score (0-1)
        
        Returns:
            Search results from all collections
        """
        url = f"{self.base_url}/search_all"
        params = {
            "query": query,
            "limit_per_collection": limit_per_collection,
            "min_score": min_score
        }
        
        response = requests.get(url, params=params)
        return response.json()
    
    def delete_collection(self, collection_name: str):
        """Delete a collection.
        
        Args:
            collection_name: Name of the collection to delete
        
        Returns:
            API response
        """
        url = f"{self.base_url}/collection/{collection_name}"
        response = requests.delete(url, headers=self._get_headers())
        
        if response.status_code == 401 and self.auth_enabled:
            # If token might be expired, try to refresh it
            self._get_token()
            response = requests.delete(url, headers=self._get_headers())
        
        return response.json()
    
    def create_user(self, username: str, password: str, disabled: bool = False):
        """Create a new user (admin only).
        
        Args:
            username: New user's username
            password: New user's password
            disabled: Whether the user should be disabled initially
            
        Returns:
            API response
        """
        if not self.auth_enabled:
            return {"message": "Authentication is disabled, user not created"}
        
        url = f"{self.base_url}/users"
        data = {
            "username": username,
            "password": password,
            "disabled": disabled
        }
        
        response = requests.post(url, json=data, headers=self._get_headers())
        
        if response.status_code == 401:
            # If token might be expired, try to refresh it
            self._get_token()
            response = requests.post(url, json=data, headers=self._get_headers())
        
        return response.json()
    
    def get_current_user(self):
        """Get current user information.
        
        Returns:
            User information
        """
        if not self.auth_enabled:
            return {"username": "anonymous", "disabled": False}
        
        url = f"{self.base_url}/users/me"
        response = requests.get(url, headers=self._get_headers())
        
        if response.status_code == 401:
            # If token might be expired, try to refresh it
            self._get_token()
            response = requests.get(url, headers=self._get_headers())
        
        return response.json()


# Example usage
if __name__ == "__main__":
    # Initialize client - it will automatically detect if auth is enabled
    client = ClassificationClient(
        base_url="http://localhost:8090",
        username="admin",  # These will be used only if auth is enabled
        password="admin"
    )
    
    # Check API status
    status = client.check_status()
    print("API Status:", json.dumps(status, indent=2))
    print(f"Authentication enabled: {client.auth_enabled}")
    
    # Create a collection
    collection_name = "unspsc_categories"
    try:
        create_result = client.create_collection(collection_name)
        print(f"Created collection: {json.dumps(create_result, indent=2)}")
    except Exception as e:
        print(f"Error creating collection: {e}")
    
    # Add some sample items
    sample_items = [
        {
            "code": "43211503",
            "name": "Notebook computer",
            "description": "A portable personal computer that typically weighs under 5 pounds.",
            "hierarchy": "Information Technology > Computer Equipment > Computers > Notebook computer",
            "metadata": {
                "category": "electronics",
                "type": "good"
            }
        },
        {
            "code": "43211507",
            "name": "Desktop computer",
            "description": "A personal computer that is designed to be used in a single location.",
            "hierarchy": "Information Technology > Computer Equipment > Computers > Desktop computer"
        }
    ]
    
    try:
        add_result = client.add_batch(sample_items, collection_name)
        print("Added items:", json.dumps(add_result, indent=2))
    except Exception as e:
        print(f"Error adding items: {e}")
    
    # Search for similar items
    search_result = client.search("laptop computer", collection_name)
    print("Search results:", json.dumps(search_result, indent=2))

File: main.py
def main():
    print("Hello from jat-ml!")


if __name__ == "__main__":
    main()


.