# rag/langgraph_vis/ws_handler.py
import logging
import asyncio
import json
import uuid
from typing import Dict, Any, Optional, List, Literal, Type

from fastapi import APIRouter, WebSocket, WebSocketDisconnect, HTTPException, Depends, Query
from starlette.websockets import WebSocketState # For checking state

# Schemas for WebSocket events and potentially initial messages
from .schemas import (
    WebSocketEventBase, # For a common structure, though specific events are better
    GraphExecutionStartEvent,
    NodeStartEvent,
    NodeEndEvent,
    EdgeTakenEvent,
    GraphExecutionEndEvent,
    GraphErrorEvent,
    ExecuteGraphRequest as WebSocketInitialMessage, # Use ExecuteGraphRequest for initial message
)

# Graph building and loading
from .core.builder import DynamicGraphBuilder, DynamicGraphBuilderError
from .core.definitions import STATIC_GRAPHS, STATE_SCHEMAS # For type hints and validation
# For loading graph definitions from files
from .api_routes import _load_graph_definition_from_file # Re-use helper from api_routes

logger = logging.getLogger(__name__)
router = APIRouter() # Can use a router or define directly on FastAPI app instance

# Helper to get a compiled graph (static or dynamic)
async def get_compiled_graph_for_execution(graph_id: str):
    """
    Loads and compiles a graph, either static or from a dynamic definition file.
    """
    if graph_id.startswith("static_"):
        static_graph_name = graph_id[len("static_"):]
        if static_graph_name in STATIC_GRAPHS:
            return STATIC_GRAPHS[static_graph_name]
        else:
            logger.warning(f"Static graph '{static_graph_name}' not found for execution.")
            raise DynamicGraphBuilderError(f"Static graph '{static_graph_name}' not found.")
    else:
        graph_def = _load_graph_definition_from_file(graph_id)
        if not graph_def:
            logger.warning(f"Graph definition ID '{graph_id}' not found for execution.")
            raise DynamicGraphBuilderError(f"Graph definition ID '{graph_id}' not found.")
        try:
            # Build the graph in a separate thread to avoid blocking the event loop if it's CPU-intensive
            # For very complex graphs, consider a process pool or task queue.
            loop = asyncio.get_running_loop()
            builder = DynamicGraphBuilder(graph_def)
            compiled_graph = await loop.run_in_executor(None, builder.build) # builder.build is synchronous
            return compiled_graph
        except DynamicGraphBuilderError as e:
            logger.error(f"Failed to build dynamic graph '{graph_id}': {e}", exc_info=True)
            raise
        except Exception as e: # Catch any other unexpected errors during build
            logger.error(f"Unexpected error building dynamic graph '{graph_id}': {e}", exc_info=True)
            raise DynamicGraphBuilderError(f"Unexpected error building graph: {str(e)}")


@router.websocket("/ws/langgraph/graphs/{graph_id}/execute") # Path matches placeholder in api_routes
async def websocket_execute_graph_legacy_path(
    websocket: WebSocket,
    graph_id: str,
    # Optional: execution_id could be passed by a client if pre-generated by HTTP trigger
    # execution_id: Optional[str] = Query(None, alias="execId"),
    # Optional: client_id for tracking connections
    # client_id: Optional[str] = Query(None)
):
    """
    WebSocket endpoint to execute a LangGraph and stream events.
    This path does not include execution_id; a new one will be generated.
    """
    # Generate a unique execution ID for this run
    current_execution_id = f"ws_exec_{uuid.uuid4().hex[:12]}"
    logger.info(f"WebSocket connection request for graph_id='{graph_id}', new execution_id='{current_execution_id}'")
    await _handle_graph_execution_websocket(websocket, graph_id, current_execution_id)

@router.websocket("/ws/langgraph/graphs/{graph_id}/execute/{execution_id}") # Path matches placeholder in api_routes
async def websocket_execute_graph_with_id(
    websocket: WebSocket,
    graph_id: str,
    execution_id: str,
    # Optional: client_id for tracking connections
    # client_id: Optional[str] = Query(None)
):
    """
    WebSocket endpoint to execute a LangGraph and stream events,
    using a pre-defined execution_id (e.g., from an HTTP trigger).
    """
    logger.info(f"WebSocket connection request for graph_id='{graph_id}', using execution_id='{execution_id}'")
    await _handle_graph_execution_websocket(websocket, graph_id, execution_id)


async def _handle_graph_execution_websocket(
    websocket: WebSocket,
    graph_id: str,
    execution_id: str
):
    """
    Core logic for handling WebSocket graph execution.
    """
    client_host = websocket.client.host if websocket.client else "unknown"
    client_port = websocket.client.port if websocket.client else "unknown"
    logger.info(f"WebSocket connection accepted for graph_id='{graph_id}', execution_id='{execution_id}' from {client_host}:{client_port}")

    await websocket.accept()

    compiled_graph = None
    initial_input_args: Dict[str, Any] = {}

    try:
        # 1. Load or Build the Graph
        try:
            compiled_graph = await get_compiled_graph_for_execution(graph_id)
            logger.info(f"Successfully loaded/built graph '{graph_id}' for execution_id '{execution_id}'.")
        except DynamicGraphBuilderError as e:
            logger.error(f"Failed to load/build graph '{graph_id}' for exec_id '{execution_id}': {e}")
            error_event = GraphErrorEvent(
                execution_id=execution_id, graph_id=graph_id, message=f"Graph loading/building error: {str(e)}"
            )
            await websocket.send_json(error_event.model_dump(mode="json"))
            await websocket.close(code=1011) # Internal error
            return
        except Exception as e: # Catch any other unexpected errors
            logger.error(f"Unexpected error preparing graph '{graph_id}' for exec_id '{execution_id}': {e}", exc_info=True)
            error_event = GraphErrorEvent(
                execution_id=execution_id, graph_id=graph_id, message=f"Unexpected server error preparing graph: {str(e)}"
            )
            await websocket.send_json(error_event.model_dump(mode="json"))
            await websocket.close(code=1011)
            return        # 2. Wait for initial message from client
        try:
            # Wait for and parse the initial message using our Pydantic model
            initial_message_raw = await asyncio.wait_for(websocket.receive_text(), timeout=10.0)
            initial_message_data_raw = json.loads(initial_message_raw)
            # Validate with Pydantic model
            initial_message = WebSocketInitialMessage(**initial_message_data_raw)
            
            initial_input_args = initial_message.input_args
            config_overrides = {}
            if initial_message.config_overrides:
                config_overrides = initial_message.config_overrides # Store for later use
            simulation_delay_ms = None
            if initial_message.simulation_delay_ms is not None:
                simulation_delay_ms = initial_message.simulation_delay_ms
            
            logger.info(f"Received initial input for exec_id '{execution_id}': args={initial_input_args}, delay_ms={simulation_delay_ms}")
        except asyncio.TimeoutError:
            logger.info(f"No initial input message received for exec_id '{execution_id}'. Using defaults.")
        except (json.JSONDecodeError, Exception) as e: # Catch Pydantic validation errors too
            logger.warning(f"Invalid initial message for exec_id '{execution_id}': {e}. Using defaults.")
        except asyncio.TimeoutError:
            logger.info(f"No initial input message received within timeout for exec_id '{execution_id}'. Proceeding with empty inputs.")
        except json.JSONDecodeError:
            logger.warning(f"Invalid JSON in initial message for exec_id '{execution_id}'. Proceeding with empty inputs.")
        except WebSocketDisconnect: # Client disconnected before sending initial message
            logger.info(f"Client disconnected before sending initial input for exec_id '{execution_id}'.")
            return # Gracefully exit
        except Exception as e:
            logger.error(f"Error processing initial message for exec_id '{execution_id}': {e}", exc_info=True)
            # Proceed with empty inputs, or send an error and close        # 3. Stream Graph Execution Events
        start_event = GraphExecutionStartEvent(
            execution_id=execution_id, graph_id=graph_id, input_args=initial_input_args
        )
        await websocket.send_json(start_event.model_dump(mode="json"))

        # Recursion limit: Important for LangGraph
        recursion_limit = 25 
        
        # Prepare config for LangGraph execution
        execution_config = {"recursion_limit": recursion_limit}
        if config_overrides: # Merge any explicit config_overrides from client
            execution_config.update(config_overrides)
        
        # THIS IS WHERE YOU PASS simulation_delay_ms TO THE GRAPH'S CONTEXT/CONFIG
        if simulation_delay_ms is not None:
            # Since nodes need to check for this value, let's add it to the execution config
            # Option 1: Pass it through the execution_config so node implementations can access it
            execution_config["simulation_delay_ms"] = simulation_delay_ms
            logger.info(f"Execution will use simulation_delay_ms: {simulation_delay_ms} (nodes must be adapted to use it from their config)")

        async for event_chunk in compiled_graph.astream_events(
            initial_input_args, 
            version="v2", 
            config=execution_config # Pass the prepared config
        ):
            event_type = event_chunk["event"]
            event_data = event_chunk.get("data", {})
            event_name = event_chunk.get("name", "") # Often the node name or runnable name
            tags = event_chunk.get("tags", [])

            # Ensure websocket is still open before sending
            if websocket.client_state != WebSocketState.CONNECTED:
                logger.warning(f"WebSocket disconnected during streaming for exec_id '{execution_id}'. Stopping stream.")
                break

            # --- Map LangGraph events to our defined WebSocket schemas ---
            # This mapping might need adjustment based on the verbosity and structure of LangGraph's `astream_events`
            # For "v2" events:
            # - "on_chain_start", "on_chain_end", "on_chain_stream"
            # - "on_llm_start", "on_llm_end", "on_llm_stream"
            # - "on_tool_start", "on_tool_end"
            # - "on_chat_model_start", ...
            # - tags: ["langgraph:node:<node_name>"], ["langgraph:edge"], ["langgraph:action"] etc.

            if "langgraph:node:"+event_name in tags: # Heuristic for identifying node-related events by tag
                if event_type == "on_chain_start" or event_type == "on_tool_start" or event_type == "on_chat_model_start": # General start events for nodes
                    node_start_ws_event = NodeStartEvent(
                        execution_id=execution_id,
                        graph_id=graph_id,
                        node_id=event_name, # `name` field from event usually corresponds to node ID
                        input_data=event_data.get("input", event_data.get("input_str", {})) # Prefer "input" if available
                    )
                    await websocket.send_json(node_start_ws_event.model_dump(mode="json"))

                elif event_type == "on_chain_end" or event_type == "on_tool_end" or event_type == "on_chat_model_end": # General end events for nodes
                    status: Literal["success", "failure"] = "success" # type: ignore
                    error_msg: Optional[str] = None
                    # LangGraph event structure for errors might vary. Check if output is an exception.
                    output = event_data.get("output", {})
                    if isinstance(output, Exception):
                        status = "failure"
                        error_msg = str(output)
                    elif isinstance(output, dict) and output.get("messages") and isinstance(output["messages"][-1], BaseException): # type: ignore
                        status = "failure"
                        error_msg = str(output["messages"][-1])

                    node_end_ws_event = NodeEndEvent(
                        execution_id=execution_id,
                        graph_id=graph_id,
                        node_id=event_name,
                        output_data=output if status == "success" else {"error": error_msg},
                        status=status,
                        error_message=error_msg
                        # duration_ms: event_data.get("duration_ms") # If LangGraph provides this directly
                    )
                    await websocket.send_json(node_end_ws_event.model_dump(mode="json"))

            # Check for edge events (experimental based on potential tag)
            # Actual edge traversal might need to be inferred or LangGraph might offer more direct events.
            # LangGraph's internal "implicit" edges (like from a node to END) might not produce an "edge" event.
            # This part is highly dependent on LangGraph's `astream_events` v2 structure for edges.
            if "langgraph:edge" in tags and event_type == "on_chain_start": # Assuming edge leads to start of next chain
                # This is a guess. `astream_events` `data` for an edge event might contain source/target.
                # We might need to maintain previous node to infer source.
                # For now, let's assume the `name` for an edge event could be 'source_id:target_id:condition'
                # Or, it might be that `event_data` gives source/target explicitly for edge events.
                # This part needs careful testing with actual `astream_events` v2 output for edges.
                # Simpler approach: infer edge from node_end -> next_node_start.
                pass # Placeholder for more robust edge event handling

            # Placeholder for sending EdgeTakenEvent (often inferred from node sequence if not explicit)

        # 4. Send Graph Execution End event
        # The final state is usually the last event_chunk's data if it's the graph's end.
        # However, `astream_events` itself doesn't give a single "final state" object directly.
        # The last relevant "on_chain_end" for the graph itself or END node would be key.
        # For now, we'll make a best guess or acknowledge completion.
        # A more robust way is to capture the final output from `ainvoke`. Here, we're streaming.
        final_state_data = event_chunk.get("data", {}).get("output", {}) if 'event_chunk' in locals() else {} # type: ignore
        graph_end_event = GraphExecutionEndEvent(
            execution_id=execution_id,
            graph_id=graph_id,
            final_state=final_state_data, # This is an approximation from the stream
            status="completed" # Assuming successful completion if loop finishes
        )
        await websocket.send_json(graph_end_event.model_dump(mode="json"))
        logger.info(f"Graph execution stream completed for exec_id '{execution_id}'.")

    except WebSocketDisconnect:
        logger.info(f"Client {client_host}:{client_port} (exec_id '{execution_id}') disconnected.")
    except DynamicGraphBuilderError as e: # Catch errors from get_compiled_graph_for_execution if not caught earlier
        logger.error(f"Graph Builder Error for exec_id '{execution_id}', graph_id '{graph_id}': {e}")
        if websocket.client_state == WebSocketState.CONNECTED:
            error_event = GraphErrorEvent(execution_id=execution_id, graph_id=graph_id, message=str(e))
            await websocket.send_json(error_event.model_dump(mode="json"))
    except Exception as e:
        logger.error(f"Unhandled error during WebSocket execution for exec_id '{execution_id}': {e}", exc_info=True)
        if websocket.client_state == WebSocketState.CONNECTED:
            error_event = GraphErrorEvent(
                execution_id=execution_id,
                graph_id=graph_id,
                message="An unexpected server error occurred during graph execution.",
                details=str(e)
            )
            await websocket.send_json(error_event.model_dump(mode="json"))
    finally:
        if websocket.client_state == WebSocketState.CONNECTED:
            await websocket.close()
        logger.info(f"WebSocket connection closed for exec_id '{execution_id}' from {client_host}:{client_port}")

# How to integrate this router into your main FastAPI app (e.g., in rag/api/app.py):
# from rag.langgraph_vis import ws_handler # Assuming this structure
# app.include_router(ws_handler.router)
# Or, if not using a router, you'd define the @app.websocket directly in your main app file
# and call _handle_graph_execution_websocket.