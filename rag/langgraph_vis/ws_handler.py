# rag/langgraph_vis/ws_handler.py
import logging
import asyncio
import json
import uuid
from typing import Dict, Any, Optional, List, Literal, Type, get_type_hints
import inspect

from fastapi import APIRouter, WebSocket, WebSocketDisconnect, HTTPException, Depends, Query
from starlette.websockets import WebSocketState # For checking state
from pydantic import BaseModel, Field
# Import langchain message types for custom JSON serialization
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage

# Schemas for WebSocket events and potentially initial messages
from .schemas import (
    WebSocketEventBase, # For a common structure, though specific events are better
    GraphExecutionStartEvent,
    NodeStartEvent,
    NodeEndEvent,
    EdgeTakenEvent,
    GraphExecutionEndEvent,
    GraphErrorEvent,
    ExecuteGraphRequest as WebSocketInitialMessage, # Use ExecuteGraphRequest for initial message
)

# Graph building and loading
from .core.builder import DynamicGraphBuilder, DynamicGraphBuilderError
from .core.definitions import STATIC_GRAPHS, STATE_SCHEMAS, STATIC_GRAPHS_METADATA # For type hints and validation
# For loading graph definitions from files
from .api_routes import _load_graph_definition_from_file # Re-use helper from api_routes

logger = logging.getLogger(__name__)
router = APIRouter() # Can use a router or define directly on FastAPI app instance

# Custom JSON encoder to handle AIMessage and other LangChain objects
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj: Any) -> Any:
        # Handle all BaseMessage objects (AIMessage, HumanMessage, etc.)
        if isinstance(obj, BaseMessage):
            return {
                "type": obj.__class__.__name__,
                "role": getattr(obj, "role", "assistant"),
                "content": obj.content,
                "additional_kwargs": obj.additional_kwargs
            }
        
        # Handle other LangChain objects with to_dict() method
        if hasattr(obj, "to_dict") and callable(getattr(obj, "to_dict")):
            return obj.to_dict()
            
        # Handle objects with __dict__ attribute
        if hasattr(obj, "__dict__"):
            return obj.__dict__
            
        # Try string representation as a last resort for other unserializable objects
        try:
            return str(obj)
        except:
            # Call the base class implementation for other types
            return super().default(obj)

# Helper to get a compiled graph (static or dynamic)
async def get_compiled_graph_for_execution(graph_id: str):
    """
    Loads and compiles a graph, either static or from a dynamic definition file.
    """
    if graph_id.startswith("static_"):
        static_graph_name = graph_id[len("static_"):]
        if static_graph_name in STATIC_GRAPHS:
            return STATIC_GRAPHS[static_graph_name]
        else:
            logger.warning(f"Static graph '{static_graph_name}' not found for execution.")
            raise DynamicGraphBuilderError(f"Static graph '{static_graph_name}' not found.")
    else:
        graph_def = _load_graph_definition_from_file(graph_id)
        if not graph_def:
            logger.warning(f"Graph definition ID '{graph_id}' not found for execution.")
            raise DynamicGraphBuilderError(f"Graph definition ID '{graph_id}' not found.")
        try:
            # Build the graph in a separate thread to avoid blocking the event loop if it's CPU-intensive
            # For very complex graphs, consider a process pool or task queue.
            loop = asyncio.get_running_loop()
            builder = DynamicGraphBuilder(graph_def)
            compiled_graph = await loop.run_in_executor(None, builder.build) # builder.build is synchronous
            return compiled_graph
        except DynamicGraphBuilderError as e:
            logger.error(f"Failed to build dynamic graph '{graph_id}': {e}", exc_info=True)
            raise
        except Exception as e: # Catch any other unexpected errors during build
            logger.error(f"Unexpected error building dynamic graph '{graph_id}': {e}", exc_info=True)
            raise DynamicGraphBuilderError(f"Unexpected error building graph: {str(e)}")


@router.websocket("/ws/langgraph/graphs/{graph_id}/execute") # Path matches placeholder in api_routes
async def websocket_execute_graph_legacy_path(
    websocket: WebSocket,
    graph_id: str,
    # Optional: execution_id could be passed by a client if pre-generated by HTTP trigger
    # execution_id: Optional[str] = Query(None, alias="execId"),
    # Optional: client_id for tracking connections
    # client_id: Optional[str] = Query(None)
):
    """
    WebSocket endpoint to execute a LangGraph and stream events.
    This path does not include execution_id; a new one will be generated.
    """
    # Generate a unique execution ID for this run
    current_execution_id = f"ws_exec_{uuid.uuid4().hex[:12]}"
    logger.info(f"WebSocket connection request for graph_id='{graph_id}', new execution_id='{current_execution_id}'")
    await _handle_graph_execution_websocket(websocket, graph_id, current_execution_id)

@router.websocket("/ws/langgraph/graphs/{graph_id}/execute/{execution_id}") # Path matches placeholder in api_routes
async def websocket_execute_graph_with_id(
    websocket: WebSocket,
    graph_id: str,
    execution_id: str,
    # Optional: client_id for tracking connections
    # client_id: Optional[str] = Query(None)
):
    """
    WebSocket endpoint to execute a LangGraph and stream events,
    using a pre-defined execution_id (e.g., from an HTTP trigger).
    """
    logger.info(f"WebSocket connection request for graph_id='{graph_id}', using execution_id='{execution_id}'")
    await _handle_graph_execution_websocket(websocket, graph_id, execution_id)


async def _handle_graph_execution_websocket(
    websocket: WebSocket,
    graph_id: str,
    execution_id: str
):
    """
    Core logic for handling WebSocket graph execution.
    """
    client_host = websocket.client.host if websocket.client else "unknown"
    client_port = websocket.client.port if websocket.client else "unknown"
    logger.info(f"WebSocket connection accepted for graph_id='{graph_id}', execution_id='{execution_id}' from {client_host}:{client_port}")

    await websocket.accept()

    compiled_graph = None
    initial_input_args: Dict[str, Any] = {}

    try:
        # 1. Load or Build the Graph
        try:
            compiled_graph = await get_compiled_graph_for_execution(graph_id)
            logger.info(f"Successfully loaded/built graph '{graph_id}' for execution_id '{execution_id}'.")
        except DynamicGraphBuilderError as e:
            logger.error(f"Failed to load/build graph '{graph_id}' for exec_id '{execution_id}': {e}")
            error_event = GraphErrorEvent(
                execution_id=execution_id, graph_id=graph_id, message=f"Graph loading/building error: {str(e)}"
            )
            # Use custom JSON encoder to handle any special objects
            json_data = json.dumps({
                "eventType": error_event.event_type,
                "timestamp": error_event.timestamp.isoformat(),
                "executionId": error_event.execution_id,
                "graphId": error_event.graph_id,
                "message": error_event.message,
                "details": error_event.details
            }, cls=CustomJSONEncoder)
            await websocket.send_text(json_data)
            await websocket.close(code=1011) # Internal error
            return
        except Exception as e: # Catch any other unexpected errors
            logger.error(f"Unexpected error preparing graph '{graph_id}' for exec_id '{execution_id}': {e}", exc_info=True)
            error_event = GraphErrorEvent(
                execution_id=execution_id, graph_id=graph_id, message=f"Unexpected server error preparing graph: {str(e)}"
            )
            # Use custom JSON encoder to handle any special objects
            json_data = json.dumps({
                "eventType": error_event.event_type,
                "timestamp": error_event.timestamp.isoformat(),
                "executionId": error_event.execution_id,
                "graphId": error_event.graph_id,
                "message": error_event.message,
                "details": error_event.details
            }, cls=CustomJSONEncoder)
            await websocket.send_text(json_data)
            await websocket.close(code=1011)
            return
            
        # 2. Wait for initial message from client
        try:
            # Wait for and parse the initial message using our Pydantic model
            initial_message_raw = await asyncio.wait_for(websocket.receive_text(), timeout=10.0)
            initial_message_data_raw = json.loads(initial_message_raw)
            # Validate with Pydantic model
            initial_message = WebSocketInitialMessage(**initial_message_data_raw)
            
            initial_input_args = initial_message.input_args
            config_overrides = {}
            if initial_message.config_overrides:
                config_overrides = initial_message.config_overrides # Store for later use
            simulation_delay_ms = None
            if initial_message.simulation_delay_ms is not None:
                simulation_delay_ms = initial_message.simulation_delay_ms
            
            logger.info(f"Received initial input for exec_id '{execution_id}': args={initial_input_args}, delay_ms={simulation_delay_ms}")
        except asyncio.TimeoutError:
            logger.info(f"No initial input message received for exec_id '{execution_id}'. Using defaults.")
        except (json.JSONDecodeError, Exception) as e: # Catch Pydantic validation errors too
            logger.warning(f"Invalid initial message for exec_id '{execution_id}': {e}. Using defaults.")
        except asyncio.TimeoutError:
            logger.info(f"No initial input message received within timeout for exec_id '{execution_id}'. Proceeding with empty inputs.")
        except json.JSONDecodeError:
            logger.warning(f"Invalid JSON in initial message for exec_id '{execution_id}'. Proceeding with empty inputs.")
        except WebSocketDisconnect: # Client disconnected before sending initial message
            logger.info(f"Client disconnected before sending initial input for exec_id '{execution_id}'.")
            return # Gracefully exit
        except Exception as e:
            logger.error(f"Error processing initial message for exec_id '{execution_id}': {e}", exc_info=True)        # Proceed with empty inputs, or send an error and close
        
        # Determine the graph's state schema type and provide default values for required fields
        state_schema_name = None
        state_schema_type = None
        
        # Special case for DocumentProcessingState - directly check if this is example_document_workflow
        if graph_id == "static_example_document_workflow" or graph_id.endswith("example_document_workflow"):
            # Ensure required original_document field is always present
            if "original_document" not in initial_input_args or not initial_input_args["original_document"]:
                initial_input_args["original_document"] = "Default document for visualization"
                logger.info(f"Added required 'original_document' field for DocumentProcessingState in graph '{graph_id}'")
        
        # Try to get state schema for static graphs from metadata
        if graph_id.startswith("static_"):
            static_graph_name = graph_id[len("static_"):]
            if static_graph_name in STATIC_GRAPHS_METADATA:
                state_schema_name = STATIC_GRAPHS_METADATA[static_graph_name].get("state_schema_name")
                if state_schema_name and state_schema_name in STATE_SCHEMAS:
                    state_schema_type = STATE_SCHEMAS[state_schema_name]
                    logger.info(f"Found state schema '{state_schema_name}' for graph '{graph_id}'")
        
        # Add required fields based on schema name
        if state_schema_name == "DocumentProcessingState":
            # Always ensure original_document is present for DocumentProcessingState
            if "original_document" not in initial_input_args or not initial_input_args["original_document"]:
                initial_input_args["original_document"] = "Default document for visualization"
                logger.info(f"Added required 'original_document' field for DocumentProcessingState")
        elif state_schema_name == "BasicAgentState":
            # For BasicAgentState, ensure messages field is present
            if "messages" not in initial_input_args:
                initial_input_args["messages"] = []
                logger.info(f"Added required 'messages' field for BasicAgentState")
        
        # For all schema types, try validation and error correction
        if state_schema_type:
            try:
                # For Pydantic models, try to validate with the model
                if issubclass(state_schema_type, BaseModel):
                    model_instance = state_schema_type(**initial_input_args)
                    logger.info(f"Successfully validated input args against {state_schema_name} schema")
            except Exception as validation_error:
                error_msg = str(validation_error)
                logger.warning(f"Validation error for {state_schema_name}: {validation_error}")
                
                # Extract field names from error message
                import re
                field_errors = re.findall(r"field required for (\w+)", error_msg.lower())
                
                for field in field_errors:
                    if field == "original_document":
                        initial_input_args[field] = "Default document added after validation error"
                    elif field == "messages":
                        initial_input_args[field] = []
                    else:
                        # Default to empty string for unknown fields
                        initial_input_args[field] = ""
                    logger.info(f"Added missing required field '{field}' based on validation error")
        
        logger.info(f"Input args after adding defaults: {initial_input_args}")

        # 3. Stream Graph Execution Events
        start_event = GraphExecutionStartEvent(
            execution_id=execution_id, graph_id=graph_id, input_args=initial_input_args
        )
        # Use custom JSON encoder to handle any special objects in input args
        json_data = json.dumps({
            "eventType": start_event.event_type,
            "timestamp": start_event.timestamp.isoformat(),
            "executionId": start_event.execution_id,
            "graphId": start_event.graph_id,
            "inputArgs": start_event.input_args
        }, cls=CustomJSONEncoder)
        await websocket.send_text(json_data)

        # Recursion limit: Important for LangGraph
        recursion_limit = 25 
        
        # Prepare config for LangGraph execution
        execution_config = {"recursion_limit": recursion_limit}
        if config_overrides: # Merge any explicit config_overrides from client
            execution_config.update(config_overrides)
        
        # THIS IS WHERE YOU PASS simulation_delay_ms TO THE GRAPH'S CONTEXT/CONFIG
        if simulation_delay_ms is not None:
            # Since nodes need to check for this value, let's add it to the execution config
            # Option 1: Pass it through the execution_config so node implementations can access it
            execution_config["simulation_delay_ms"] = simulation_delay_ms
            logger.info(f"Execution will use simulation_delay_ms: {simulation_delay_ms} (nodes must be adapted to use it from their config)")

        async for event_chunk in compiled_graph.astream_events(
            initial_input_args, 
            version="v2", 
            config=execution_config # Pass the prepared config
        ):
            event_type = event_chunk["event"]
            event_data = event_chunk.get("data", {})
            event_name = event_chunk.get("name", "") # Often the node name or runnable name
            tags = event_chunk.get("tags", [])

            # Ensure websocket is still open before sending
            if websocket.client_state != WebSocketState.CONNECTED:
                logger.warning(f"WebSocket disconnected during streaming for exec_id '{execution_id}'. Stopping stream.")
                break

            # --- Map LangGraph events to our defined WebSocket schemas ---
            # This mapping might need adjustment based on the verbosity and structure of LangGraph's `astream_events`
            # For "v2" events:
            # - "on_chain_start", "on_chain_end", "on_chain_stream"
            # - "on_llm_start", "on_llm_end", "on_llm_stream"
            # - "on_tool_start", "on_tool_end"
            # - "on_chat_model_start", ...
            # - tags: ["langgraph:node:<node_name>"], ["langgraph:edge"], ["langgraph:action"] etc.

            if "langgraph:node:"+event_name in tags: # Heuristic for identifying node-related events by tag
                if event_type == "on_chain_start" or event_type == "on_tool_start" or event_type == "on_chat_model_start": # General start events for nodes
                    node_start_ws_event = NodeStartEvent(
                        execution_id=execution_id,
                        graph_id=graph_id,
                        node_id=event_name, # `name` field from event usually corresponds to node ID
                        input_data=event_data.get("input", event_data.get("input_str", {})) # Prefer "input" if available
                    )
                    # Use custom JSON encoder to handle AIMessage objects
                    json_data = json.dumps({
                        "eventType": node_start_ws_event.event_type,
                        "timestamp": node_start_ws_event.timestamp.isoformat(),
                        "executionId": node_start_ws_event.execution_id,
                        "graphId": node_start_ws_event.graph_id,
                        "nodeId": node_start_ws_event.node_id,
                        "nodeType": node_start_ws_event.node_type,
                        "inputData": node_start_ws_event.input_data
                    }, cls=CustomJSONEncoder)
                    await websocket.send_text(json_data)

                elif event_type == "on_chain_end" or event_type == "on_tool_end" or event_type == "on_chat_model_end": # General end events for nodes
                    status: Literal["success", "failure"] = "success" # type: ignore
                    error_msg: Optional[str] = None
                    # LangGraph event structure for errors might vary. Check if output is an exception.
                    output = event_data.get("output", {})
                    if isinstance(output, Exception):
                        status = "failure"
                        error_msg = str(output)
                    elif isinstance(output, dict) and output.get("messages") and isinstance(output["messages"][-1], BaseException): # type: ignore
                        status = "failure"
                        error_msg = str(output["messages"][-1])

                    node_end_ws_event = NodeEndEvent(
                        execution_id=execution_id,
                        graph_id=graph_id,
                        node_id=event_name,
                        output_data=output if status == "success" else {"error": error_msg},
                        status=status,
                        error_message=error_msg
                        # duration_ms: event_data.get("duration_ms") # If LangGraph provides this directly
                    )
                    # Use custom JSON encoder to handle AIMessage objects
                    json_data = json.dumps({
                        "eventType": node_end_ws_event.event_type,
                        "timestamp": node_end_ws_event.timestamp.isoformat(),
                        "executionId": node_end_ws_event.execution_id,
                        "graphId": node_end_ws_event.graph_id,
                        "nodeId": node_end_ws_event.node_id,
                        "nodeType": node_end_ws_event.node_type,
                        "outputData": node_end_ws_event.output_data,
                        "status": node_end_ws_event.status,
                        "errorMessage": node_end_ws_event.error_message,
                        "durationMs": node_end_ws_event.duration_ms
                    }, cls=CustomJSONEncoder)
                    await websocket.send_text(json_data)

            # Check for edge events (experimental based on potential tag)
            # Actual edge traversal might need to be inferred or LangGraph might offer more direct events.
            # LangGraph's internal "implicit" edges (like from a node to END) might not produce an "edge" event.
            # This part is highly dependent on LangGraph's `astream_events` v2 structure for edges.
            if "langgraph:edge" in tags and event_type == "on_chain_start": # Assuming edge leads to start of next chain
                # This is a guess. `astream_events` `data` for an edge event might contain source/target.
                # We might need to maintain previous node to infer source.
                # For now, let's assume the `name` for an edge event could be 'source_id:target_id:condition'
                # Or, it might be that `event_data` gives source/target explicitly for edge events.
                # This part needs careful testing with actual `astream_events` v2 output for edges.
                # Simpler approach: infer edge from node_end -> next_node_start.
                pass # Placeholder for more robust edge event handling

            # Placeholder for sending EdgeTakenEvent (often inferred from node sequence if not explicit)

        # 4. Send Graph Execution End event
        # The final state is usually the last event_chunk's data if it's the graph's end.
        # However, `astream_events` itself doesn't give a single "final state" object directly.
        # The last relevant "on_chain_end" for the graph itself or END node would be key.
        # For now, we'll make a best guess or acknowledge completion.
        # A more robust way is to capture the final output from `ainvoke`. Here, we're streaming.
        final_state_data = event_chunk.get("data", {}).get("output", {}) if 'event_chunk' in locals() else {} # type: ignore
        graph_end_event = GraphExecutionEndEvent(
            execution_id=execution_id,
            graph_id=graph_id,
            final_state=final_state_data, # This is an approximation from the stream
            status="completed" # Assuming successful completion if loop finishes
        )
        # Use custom JSON encoder to handle AIMessage objects
        json_data = json.dumps({
            "eventType": graph_end_event.event_type,
            "timestamp": graph_end_event.timestamp.isoformat(),
            "executionId": graph_end_event.execution_id,
            "graphId": graph_end_event.graph_id,
            "finalState": graph_end_event.final_state,
            "status": graph_end_event.status,
            "totalDurationMs": graph_end_event.total_duration_ms
        }, cls=CustomJSONEncoder)
        await websocket.send_text(json_data)
        logger.info(f"Graph execution stream completed for exec_id '{execution_id}'.")

    except WebSocketDisconnect:
        logger.info(f"Client {client_host}:{client_port} (exec_id '{execution_id}') disconnected.")
    except DynamicGraphBuilderError as e: # Catch errors from get_compiled_graph_for_execution if not caught earlier
        logger.error(f"Graph Builder Error for exec_id '{execution_id}', graph_id '{graph_id}': {e}")
        if websocket.client_state == WebSocketState.CONNECTED:
            error_event = GraphErrorEvent(execution_id=execution_id, graph_id=graph_id, message=str(e))
            # Use custom JSON encoder to handle any special objects
            json_data = json.dumps({
                "eventType": error_event.event_type,
                "timestamp": error_event.timestamp.isoformat(),
                "executionId": error_event.execution_id,
                "graphId": error_event.graph_id,
                "message": error_event.message,
                "details": error_event.details
            }, cls=CustomJSONEncoder)
            await websocket.send_text(json_data)
    except Exception as e:
        logger.error(f"Unhandled error during WebSocket execution for exec_id '{execution_id}': {e}", exc_info=True)
        if websocket.client_state == WebSocketState.CONNECTED:
            error_event = GraphErrorEvent(
                execution_id=execution_id,
                graph_id=graph_id,
                message="An unexpected server error occurred during graph execution.",
                details=str(e)
            )
            # Use custom JSON encoder to handle any special objects
            json_data = json.dumps({
                "eventType": error_event.event_type,
                "timestamp": error_event.timestamp.isoformat(),
                "executionId": error_event.execution_id,
                "graphId": error_event.graph_id,
                "message": error_event.message,
                "details": error_event.details
            }, cls=CustomJSONEncoder)
            await websocket.send_text(json_data)
    finally:
        if websocket.client_state == WebSocketState.CONNECTED:
            await websocket.close()
        logger.info(f"WebSocket connection closed for exec_id '{execution_id}' from {client_host}:{client_port}")

# How to integrate this router into your main FastAPI app (e.g., in rag/api/app.py):
# from rag.langgraph_vis import ws_handler # Assuming this structure
# app.include_router(ws_handler.router)
# Or, if not using a router, you'd define the @app.websocket directly in your main app file
# and call _handle_graph_execution_websocket.
